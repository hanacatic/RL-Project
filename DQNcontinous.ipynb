{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import random\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Replay Memory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',('state', 'action', 'next_state', 'reward', 'done'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = deque([], maxlen = self.capacity) # cyclic buffer of bounded size\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "    \n",
    "    def push(self, *args):\n",
    "        self.memory.append(Transition(*args))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DQN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, n_observations, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.layers = torch.nn.Sequential(\n",
    "            torch.nn.Linear(n_observations, 256),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(256,256), # only one hidden layer, the first two regarding the convolution as processing of input images are dropped\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(256, n_actions)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Environment***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"MountainCarContinuous-v0\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.99\n",
    "EPS_START = 0.99\n",
    "EPS_END = 0.001\n",
    "EPS_DECAY = 0.9\n",
    "REPLAY_MEMORY = 100000\n",
    "LR = 0.0005 #1e-4\n",
    "GRID_SIZE = 20\n",
    "disc_action_space = np.linspace(-1.0, 1.0, GRID_SIZE)\n",
    "\n",
    "state, info = env.reset()\n",
    "n_actions = GRID_SIZE  # continuous action space with 20 discrete actions #env.action_space.n for discrete action space\n",
    "n_observations = len(state)\n",
    "\n",
    "memory = ReplayMemory(REPLAY_MEMORY)\n",
    "\n",
    "estimate_network = DQN(n_observations, n_actions)\n",
    "target_network = DQN(n_observations, n_actions)\n",
    "target_network.load_state_dict(estimate_network.state_dict())\n",
    "\n",
    "optimizer = torch.optim.AdamW(estimate_network.parameters(), lr = LR)\n",
    "\n",
    "episode_durations = []\n",
    "\n",
    "def discretize_action(action):\n",
    "    i=np.argmin(np.abs(disc_action_space - action))\n",
    "    return [disc_action_space[i]]\n",
    "\n",
    "def act(state, eps):\n",
    "    if random.random() < eps:\n",
    "        return torch.tensor([[discretize_action(env.action_space.sample())]], device=device, dtype=torch.long)\n",
    "    else: \n",
    "        with torch.no_grad(): # improves computation\n",
    "            return estimate_network(state).max(1).indices.view(1,1)\n",
    "        \n",
    "def plot_durations(show_result=False):\n",
    "    plt.figure(1)\n",
    "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
    "    if show_result:\n",
    "        plt.title('Result')\n",
    "    else:\n",
    "        plt.clf()\n",
    "        plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(durations_t.numpy())\n",
    "    # Take 100 episode averages and plot them too\n",
    "    if len(durations_t) >= 100:\n",
    "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy())\n",
    "\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "    if is_ipython:\n",
    "        if not show_result:\n",
    "            display.display(plt.gcf())\n",
    "            display.clear_output(wait=True)\n",
    "        else:\n",
    "            display.display(plt.gcf())\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\leoga\\AppData\\Local\\Temp\\ipykernel_12116\\1605881464.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  action = torch.tensor(action).to(device) # convert to tensor and squisch to 1D #before torch.tensor(action).to(device)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'int' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m action \u001b[38;5;241m=\u001b[39m act(state, eps) \u001b[38;5;66;03m# epsilon greedy \u001b[39;00m\n\u001b[0;32m     12\u001b[0m action \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(action)\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;66;03m# convert to tensor and squisch to 1D #before torch.tensor(action).to(device)\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m next_state, reward, terminated, truncated, _  \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# execute action\u001b[39;00m\n\u001b[0;32m     15\u001b[0m done \u001b[38;5;241m=\u001b[39m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n\u001b[0;32m     16\u001b[0m episode_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\gymnasium\\wrappers\\time_limit.py:57\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[0;32m     47\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \n\u001b[0;32m     49\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     55\u001b[0m \n\u001b[0;32m     56\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 57\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\gymnasium\\wrappers\\order_enforcing.py:56\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\gymnasium\\wrappers\\env_checker.py:51\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\gymnasium\\envs\\classic_control\\continuous_mountain_car.py:149\u001b[0m, in \u001b[0;36mContinuous_MountainCarEnv.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    147\u001b[0m position \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    148\u001b[0m velocity \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m--> 149\u001b[0m force \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mmax\u001b[39m(\u001b[43maction\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_action), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_action)\n\u001b[0;32m    151\u001b[0m velocity \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m force \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpower \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m0.0025\u001b[39m \u001b[38;5;241m*\u001b[39m math\u001b[38;5;241m.\u001b[39mcos(\u001b[38;5;241m3\u001b[39m \u001b[38;5;241m*\u001b[39m position)\n\u001b[0;32m    152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m velocity \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_speed:\n",
      "\u001b[1;31mTypeError\u001b[0m: 'int' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "episode_rewards = []\n",
    "successful_sequences = 0\n",
    "eps = EPS_START\n",
    "for ep in range(1000):\n",
    "    state, info = env.reset() # info in some environments has some additional information\n",
    "    state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "    \n",
    "    while not done:\n",
    "        action = act(state, eps) # epsilon greedy \n",
    "        action = torch.tensor(action).to(device) # convert to tensor and squisch to 1D #before torch.tensor(action).to(device)\n",
    "        \n",
    "        next_state, reward, terminated, truncated, _  = env.step(action.item()) # execute action\n",
    "        done = terminated or truncated\n",
    "        episode_reward += reward\n",
    "        \n",
    "        \n",
    "        next_state = torch.tensor(next_state, dtype=torch.float32, device=device).unsqueeze(0) # get next state\n",
    "        reward = torch.tensor(reward).to(device).float().unsqueeze(0) \n",
    "        done = torch.tensor(done).to(device).unsqueeze(0)\n",
    "\n",
    "        memory.push(state, action, next_state, reward, done) # store transition in replay memory\n",
    "        \n",
    "        state = next_state\n",
    "        \n",
    "        if len(memory) > BATCH_SIZE: \n",
    "            transitions = memory.sample(BATCH_SIZE) # sample batch from replay memory\n",
    "            batch = Transition(*zip(*transitions))\n",
    "\n",
    "            next_state_batch = torch.cat(batch.next_state)\n",
    "            state_batch = torch.cat(batch.state)\n",
    "            action_batch = torch.cat(batch.action)\n",
    "            reward_batch = torch.cat(batch.reward)\n",
    "            done_batch = torch.cat(batch.done)\n",
    "            \n",
    "            # determine if next state is terminal state (two approaches, one add done as part of the transition to the replay memory and second to try to determine it)\n",
    "            # non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), device=device, dtype=torch.bool)\n",
    "            # but this will only check if it s terminal in the batch and not overall the influnce of this would be interesting to check for sure\n",
    "            \n",
    "            y = reward_batch + (done_batch == False) * GAMMA * target_network(next_state_batch).max(1).values # compute target\n",
    "            \n",
    "            criterion = torch.nn.SmoothL1Loss() # criterion defined as Huber loss less sensitive to outliers\n",
    "            loss = criterion(y, estimate_network(state_batch).gather(1, action_batch).squeeze(1)) # compute the loss\n",
    "            \n",
    "            optimizer.zero_grad() # not to accumulate gradients\n",
    "            loss.backward() # backpropagate the loss\n",
    "            for params in estimate_network.parameters():\n",
    "                params.grad.data.clamp_(-1,1)\n",
    "            # torch.nn.utils.clip_grad(estimate_network.parameters(), 1)  # why is clipping the parameters important do they get out of hand?       \n",
    "            optimizer.step()\n",
    "            # updating the target network\n",
    "            # one approach soft update and second approach delayed update to estimated network parameters\n",
    "            \n",
    "    if ep % 5 == 0:\n",
    "        target_network.load_state_dict(estimate_network.state_dict())\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "    eps = max(EPS_END, eps*EPS_DECAY) # decay epsilon\n",
    "    episode_rewards.append(episode_reward) \n",
    "    \n",
    "\n",
    "    if ep % 25 == 0: \n",
    "        print('episode', ep, ':', np.mean(episode_rewards[-25:]), 'average score')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m scores \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m ep_count \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m50\u001b[39m:\n\u001b[1;32m---> 11\u001b[0m     \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m     action \u001b[38;5;241m=\u001b[39m act(state, \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     13\u001b[0m     state, reward, terminated, truncated, _  \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action\u001b[38;5;241m.\u001b[39mitem()) \u001b[38;5;66;03m# execute action\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\gymnasium\\core.py:471\u001b[0m, in \u001b[0;36mWrapper.render\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    469\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrender\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m RenderFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mlist\u001b[39m[RenderFrame] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    470\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Uses the :meth:`render` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 471\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\gymnasium\\wrappers\\order_enforcing.py:70\u001b[0m, in \u001b[0;36mOrderEnforcing.render\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_disable_render_order_enforcing \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\n\u001b[0;32m     67\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call `env.render()` before calling `env.reset()`, if this is a intended action, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     68\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset `disable_render_order_enforcing=True` on the OrderEnforcer wrapper.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     69\u001b[0m     )\n\u001b[1;32m---> 70\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\gymnasium\\wrappers\\env_checker.py:67\u001b[0m, in \u001b[0;36mPassiveEnvChecker.render\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_render_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 67\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\gymnasium\\envs\\classic_control\\mountain_car.py:262\u001b[0m, in \u001b[0;36mMountainCarEnv.render\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    251\u001b[0m gfxdraw\u001b[38;5;241m.\u001b[39maapolygon(\n\u001b[0;32m    252\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msurf,\n\u001b[0;32m    253\u001b[0m     [(flagx, flagy2), (flagx, flagy2 \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m10\u001b[39m), (flagx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m25\u001b[39m, flagy2 \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m5\u001b[39m)],\n\u001b[0;32m    254\u001b[0m     (\u001b[38;5;241m204\u001b[39m, \u001b[38;5;241m204\u001b[39m, \u001b[38;5;241m0\u001b[39m),\n\u001b[0;32m    255\u001b[0m )\n\u001b[0;32m    256\u001b[0m gfxdraw\u001b[38;5;241m.\u001b[39mfilled_polygon(\n\u001b[0;32m    257\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msurf,\n\u001b[0;32m    258\u001b[0m     [(flagx, flagy2), (flagx, flagy2 \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m10\u001b[39m), (flagx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m25\u001b[39m, flagy2 \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m5\u001b[39m)],\n\u001b[0;32m    259\u001b[0m     (\u001b[38;5;241m204\u001b[39m, \u001b[38;5;241m204\u001b[39m, \u001b[38;5;241m0\u001b[39m),\n\u001b[0;32m    260\u001b[0m )\n\u001b[1;32m--> 262\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msurf \u001b[38;5;241m=\u001b[39m \u001b[43mpygame\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflip\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msurf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    263\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscreen\u001b[38;5;241m.\u001b[39mblit(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msurf, (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m))\n\u001b[0;32m    264\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env.close()\n",
    "env = gym.make(\"MountainCar-v0\", render_mode=\"human\")\n",
    "\n",
    "state, _ = env.reset()\n",
    "\n",
    "state = torch.tensor(state).to(device).float().unsqueeze(0)\n",
    "ep_count = 0\n",
    "current_ep_reward = 0\n",
    "scores = []\n",
    "while ep_count < 50:\n",
    "    env.render()\n",
    "    action = act(state, 0)\n",
    "    state, reward, terminated, truncated, _  = env.step(action.item()) # execute action\n",
    "    state = torch.tensor(state).to(device).float().unsqueeze(0)\n",
    "    current_ep_reward += reward\n",
    "    done = terminated or truncated \n",
    "    \n",
    "    if done:\n",
    "        ep_count += 1\n",
    "        scores.append(current_ep_reward)\n",
    "        current_ep_reward = 0\n",
    "        state, _ = env.reset()\n",
    "        state = torch.tensor(state).to(device).float().unsqueeze(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-114.0, -115.0, -116.0, -113.0, -115.0, -118.0, -112.0, -114.0, -116.0, -116.0, -121.0, -114.0, -118.0, -114.0, -120.0, -113.0, -112.0, -114.0, -115.0, -116.0, -113.0, -120.0, -113.0, -112.0, -114.0, -114.0, -112.0, -112.0, -114.0, -112.0, -114.0, -114.0, -112.0, -116.0, -120.0, -112.0, -113.0, -116.0, -116.0, -114.0, -114.0, -113.0, -116.0, -112.0, -112.0, -117.0, -113.0, -116.0, -112.0, -121.0]\n"
     ]
    }
   ],
   "source": [
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
