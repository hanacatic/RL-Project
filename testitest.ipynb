{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[WinError 126] The specified module could not be found. Error loading \"C:\\Users\\leoga\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\lib\\shm.dll\" or one of its dependencies.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmath\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01moptim\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\__init__.py:141\u001b[0m\n\u001b[0;32m    139\u001b[0m                 err \u001b[38;5;241m=\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mWinError(ctypes\u001b[38;5;241m.\u001b[39mget_last_error())\n\u001b[0;32m    140\u001b[0m                 err\u001b[38;5;241m.\u001b[39mstrerror \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m Error loading \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdll\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m or one of its dependencies.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 141\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[0;32m    143\u001b[0m     kernel32\u001b[38;5;241m.\u001b[39mSetErrorMode(prev_error_mode)\n\u001b[0;32m    146\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_preload_cuda_deps\u001b[39m(lib_folder, lib_name):\n",
      "\u001b[1;31mOSError\u001b[0m: [WinError 126] The specified module could not be found. Error loading \"C:\\Users\\leoga\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\lib\\shm.dll\" or one of its dependencies."
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import math\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "\n",
    "from dqn_models import DQN, ReplayMemory, Transition\n",
    "from utils import get_screen\n",
    "\n",
    "# environment: 'MountainCar-v0' or 'MountainCarContinuous-v0'\n",
    "env_name = 'MountainCar-v0' \n",
    "device = torch.device(\"cpu\")\n",
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.999\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 200\n",
    "TARGET_UPDATE = 10\n",
    "num_episoeds = 50\n",
    "\n",
    "resize = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    T.ToPILImage(),\n",
    "    T.Resize(40, interpolation=Image.CUBIC),\n",
    "    T.ToTensor()\n",
    "])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(env_name)\n",
    "    init_screen = get_screen(env)\n",
    "    screen_height, screen_width, _ = init_screen.shape\n",
    "    \n",
    "    # Get number of actions from gym action space\n",
    "    n_actions = env.action_space.n\n",
    "    \n",
    "    policy_net = DQN(screen_height, screen_width, n_actions).to(device)\n",
    "    target_net = DQN(screen_height, screen_width, n_actions).to(device)\n",
    "    target_net.load_state_dict(policy_net.state_dict())\n",
    "    target_net.eval()\n",
    "    \n",
    "    optimizer = optim.RMSprop(policy_net.parameters())\n",
    "    memory = ReplayMemory(10000)\n",
    "    \n",
    "    steps_done = 0\n",
    "    \n",
    "    episode_durations = []\n",
    "    \n",
    "    for i_episode in range(num_episodes):\n",
    "        # Initialize the environment and state\n",
    "        env.reset()\n",
    "        last_screen = resize(get_screen(env)).unsqueeze(0).to(device)\n",
    "        current_screen = resize(get_screen(env)).unsqueeze(0).to(device)\n",
    "        state = current_screen - last_screen\n",
    "        for t in count():\n",
    "            # Select and perform an action\n",
    "            # action = select_action(state)\n",
    "            sample = random.random()\n",
    "            eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "                math.exp(-1. * steps_done / EPS_DECAY)\n",
    "            steps_done += 1\n",
    "            if sample > eps_threshold:\n",
    "                with torch.no_grad:\n",
    "                    # t.max(1) will return largest column value of each row.\n",
    "                    # second column on max result is index of where max element was\n",
    "                    # found, so we pick action with the larger expected reward.\n",
    "                    action = policy_net(state).max(1)[1].view(1, 1).item()\n",
    "            else:\n",
    "                action = random.randrange(n_actions)\n",
    "            \n",
    "            _, reward, done, _ = env.step(action)\n",
    "            reward = torch.tensor([reward], device=device)\n",
    "            \n",
    "            # Observe new state\n",
    "            last_scrren = current_screen\n",
    "            current_screen = resize(get_screen(env)).unsqueeze(0).to(device)\n",
    "            if not done:\n",
    "                next_state = current_screen - last_screen\n",
    "            else:\n",
    "                next_state = None\n",
    "            \n",
    "            # Store the transition in memory\n",
    "            memory.push(state, action, next_state, reward)\n",
    "            \n",
    "            # Move to the next state\n",
    "            state = next_state\n",
    "            \n",
    "            # Perform one step of optimization (on the target network)\n",
    "            if len(memory) < BATCH_SIZE:\n",
    "                pass\n",
    "            else:\n",
    "                transitions = memory.sample(BATCH_SIZE)\n",
    "                # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "                # detailed explanation). This converts batch-array of Transitions\n",
    "                # to Transition of batch-arrays.\n",
    "                batch = Transition(*zip(*transitions))\n",
    "                \n",
    "                # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "                # columns of actions taken. These are the actions which would've been taken\n",
    "                # for each batch state according to policy_net\n",
    "                state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "                \n",
    "                # Compute V(s_{t+1}) for all next states.\n",
    "                # Expected values of actions for non_final_next_states are computed based\n",
    "                # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "                # This is merged based on the mask, such that we'll have either the expected\n",
    "                # state value or 0 in case the state was final.\n",
    "                next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "                next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
    "                # Compute the expected Q values\n",
    "                expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "                \n",
    "                loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "                \n",
    "                # Optimize the model\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                for param in policy_net.parameters():\n",
    "                    param.grad.data.clamp_(-1, 1)\n",
    "                optimizer.step()\n",
    "                \n",
    "            if done:\n",
    "                break            \n",
    "    # Update the target network, copying all weights and biases in DQN\n",
    "    if i_episode % TARGET_UPDATE == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
