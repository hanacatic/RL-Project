{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TD3**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "python main.py \\\n",
    "--policy \"TD3\" \\\n",
    "--env \"MountainCar-v0\" \\\n",
    "-- seed $i \\\n",
    "--start_timesteps 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORTS\n",
    "\n",
    "# TD3\n",
    "import copy\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Main\n",
    "import numpy as np\n",
    "import torch\n",
    "import gym\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "import utils\n",
    "#import TD3\n",
    "import OurDDPG\n",
    "import DDPG\n",
    "\n",
    "# Visualization\n",
    "import gym\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TD3 FILE\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Implementation of Twin Delayed Deep Deterministic Policy Gradients (TD3)\n",
    "# Paper: https://arxiv.org/abs/1802.09477\n",
    "\n",
    "\n",
    "class Actor(nn.Module):\n",
    "\tdef __init__(self, state_dim, action_dim, max_action):\n",
    "\t\tsuper(Actor, self).__init__()\n",
    "\n",
    "\t\tself.l1 = nn.Linear(state_dim, 256)\n",
    "\t\tself.l2 = nn.Linear(256, 256)\n",
    "\t\tself.l3 = nn.Linear(256, action_dim)\n",
    "\t\t\n",
    "\t\tself.max_action = max_action\n",
    "\t\t\n",
    "\n",
    "\tdef forward(self, state):\n",
    "\t\ta = F.relu(self.l1(state))\n",
    "\t\ta = F.relu(self.l2(a))\n",
    "\t\treturn self.max_action * torch.tanh(self.l3(a))\n",
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "\tdef __init__(self, state_dim, action_dim):\n",
    "\t\tsuper(Critic, self).__init__()\n",
    "\n",
    "\t\t# Q1 architecture\n",
    "\t\tself.l1 = nn.Linear(state_dim + action_dim, 256)\n",
    "\t\tself.l2 = nn.Linear(256, 256)\n",
    "\t\tself.l3 = nn.Linear(256, 1)\n",
    "\n",
    "\t\t# Q2 architecture\n",
    "\t\tself.l4 = nn.Linear(state_dim + action_dim, 256)\n",
    "\t\tself.l5 = nn.Linear(256, 256)\n",
    "\t\tself.l6 = nn.Linear(256, 1)\n",
    "\n",
    "\n",
    "\tdef forward(self, state, action):\n",
    "\t\tsa = torch.cat([state, action], 1)\n",
    "\n",
    "\t\tq1 = F.relu(self.l1(sa))\n",
    "\t\tq1 = F.relu(self.l2(q1))\n",
    "\t\tq1 = self.l3(q1)\n",
    "\n",
    "\t\tq2 = F.relu(self.l4(sa))\n",
    "\t\tq2 = F.relu(self.l5(q2))\n",
    "\t\tq2 = self.l6(q2)\n",
    "\t\treturn q1, q2\n",
    "\n",
    "\n",
    "\tdef Q1(self, state, action):\n",
    "\t\tsa = torch.cat([state, action], 1)\n",
    "\n",
    "\t\tq1 = F.relu(self.l1(sa))\n",
    "\t\tq1 = F.relu(self.l2(q1))\n",
    "\t\tq1 = self.l3(q1)\n",
    "\t\treturn q1\n",
    "\n",
    "\n",
    "class TD3(object):\n",
    "\tdef __init__(\n",
    "\t\tself,\n",
    "\t\tstate_dim,\n",
    "\t\taction_dim,\n",
    "\t\tmax_action,\n",
    "\t\tdiscount=0.99,\n",
    "\t\ttau=0.005,\n",
    "\t\tpolicy_noise=0.2,\n",
    "\t\tnoise_clip=0.5,\n",
    "\t\tpolicy_freq=2\n",
    "\t):\n",
    "\n",
    "\t\tself.actor = Actor(state_dim, action_dim, max_action).to(device)\n",
    "\t\tself.actor_target = copy.deepcopy(self.actor)\n",
    "\t\tself.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=3e-4)\n",
    "\n",
    "\t\tself.critic = Critic(state_dim, action_dim).to(device)\n",
    "\t\tself.critic_target = copy.deepcopy(self.critic)\n",
    "\t\tself.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=3e-4)\n",
    "\n",
    "\t\tself.max_action = max_action\n",
    "\t\tself.discount = discount\n",
    "\t\tself.tau = tau\n",
    "\t\tself.policy_noise = policy_noise\n",
    "\t\tself.noise_clip = noise_clip\n",
    "\t\tself.policy_freq = policy_freq\n",
    "\n",
    "\t\tself.total_it = 0\n",
    "\n",
    "\n",
    "\tdef select_action(self, state):\n",
    "\t\tstate = torch.FloatTensor(state.reshape(1, -1)).to(device)\n",
    "\t\treturn self.actor(state).cpu().data.numpy().flatten()\n",
    "\n",
    "\n",
    "\tdef train(self, replay_buffer, batch_size=256):\n",
    "\t\tself.total_it += 1\n",
    "\n",
    "\t\t# Sample replay buffer \n",
    "\t\tstate, action, next_state, reward, not_done = replay_buffer.sample(batch_size)\n",
    "\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\t# Select action according to policy and add clipped noise\n",
    "\t\t\tnoise = (\n",
    "\t\t\t\ttorch.randn_like(action) * self.policy_noise\n",
    "\t\t\t).clamp(-self.noise_clip, self.noise_clip)\n",
    "\t\t\t\n",
    "\t\t\tnext_action = (\n",
    "\t\t\t\tself.actor_target(next_state) + noise\n",
    "\t\t\t).clamp(-self.max_action, self.max_action)\n",
    "\n",
    "\t\t\t# Compute the target Q value\n",
    "\t\t\ttarget_Q1, target_Q2 = self.critic_target(next_state, next_action)\n",
    "\t\t\ttarget_Q = torch.min(target_Q1, target_Q2)\n",
    "\t\t\ttarget_Q = reward + not_done * self.discount * target_Q\n",
    "\n",
    "\t\t# Get current Q estimates\n",
    "\t\tcurrent_Q1, current_Q2 = self.critic(state, action)\n",
    "\n",
    "\t\t# Compute critic loss\n",
    "\t\tcritic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q)\n",
    "\n",
    "\t\t# Optimize the critic\n",
    "\t\tself.critic_optimizer.zero_grad()\n",
    "\t\tcritic_loss.backward()\n",
    "\t\tself.critic_optimizer.step()\n",
    "\n",
    "\t\t# Delayed policy updates\n",
    "\t\tif self.total_it % self.policy_freq == 0:\n",
    "\n",
    "\t\t\t# Compute actor losse\n",
    "\t\t\tactor_loss = -self.critic.Q1(state, self.actor(state)).mean()\n",
    "\t\t\t\n",
    "\t\t\t# Optimize the actor \n",
    "\t\t\tself.actor_optimizer.zero_grad()\n",
    "\t\t\tactor_loss.backward()\n",
    "\t\t\tself.actor_optimizer.step()\n",
    "\n",
    "\t\t\t# Update the frozen target models\n",
    "\t\t\tfor param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
    "\t\t\t\ttarget_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "\n",
    "\t\t\tfor param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
    "\t\t\t\ttarget_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "\n",
    "\n",
    "\tdef save(self, filename):\n",
    "\t\ttorch.save(self.critic.state_dict(), filename + \"_critic\")\n",
    "\t\ttorch.save(self.critic_optimizer.state_dict(), filename + \"_critic_optimizer\")\n",
    "\t\t\n",
    "\t\ttorch.save(self.actor.state_dict(), filename + \"_actor\")\n",
    "\t\ttorch.save(self.actor_optimizer.state_dict(), filename + \"_actor_optimizer\")\n",
    "\n",
    "\n",
    "\tdef load(self, filename):\n",
    "\t\tself.critic.load_state_dict(torch.load(filename + \"_critic\"))\n",
    "\t\tself.critic_optimizer.load_state_dict(torch.load(filename + \"_critic_optimizer\"))\n",
    "\t\tself.critic_target = copy.deepcopy(self.critic)\n",
    "\n",
    "\t\tself.actor.load_state_dict(torch.load(filename + \"_actor\"))\n",
    "\t\tself.actor_optimizer.load_state_dict(torch.load(filename + \"_actor_optimizer\"))\n",
    "\t\tself.actor_target = copy.deepcopy(self.actor)\n",
    "\t\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters settings\n",
    "\n",
    "class Args:\n",
    "    policy = \"TD3\"                  # Policy name (TD3, DDPG, or OurDDPG)\n",
    "    env = \"MountainCarContinuous-v0\"          # OpenAI gym environment name\n",
    "    seed = 0                        # Sets Gym, PyTorch, and Numpy seeds\n",
    "    start_timesteps = 25000         # Time steps initial random policy is used\n",
    "    eval_freq = 5000                # How often (time steps) we evaluate\n",
    "    max_timesteps = 1000000         # Max time steps to run environment\n",
    "    expl_noise = 0.1                # Std of Gaussian exploration noise\n",
    "    batch_size = 256                # Batch size for both actor and critic\n",
    "    discount = 0.99                 # Discount factor\n",
    "    tau = 0.005                     # Target network update rate\n",
    "    policy_noise = 0.1 #0.2              # Noise added to target policy during critic update\n",
    "    noise_clip = 0.5                # Range to clip target policy noise\n",
    "    policy_freq = 2                 # Frequency of delayed policy updates\n",
    "    save_model = False              # Save model and optimizer parameters\n",
    "    load_model = \"\"                 # Model load file name, \"\" doesn't load, \"default\" uses file_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "Policy: TD3, Env: MountainCarContinuous-v0, Seed: 0\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\simon\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gym\\core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "c:\\Users\\simon\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gym\\wrappers\\step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "c:\\Users\\simon\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gym\\core.py:256: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.\u001b[0m\n",
      "  deprecation(\n",
      "c:\\Users\\simon\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gym\\core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "c:\\Users\\simon\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gym\\wrappers\\step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "c:\\Users\\simon\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gym\\core.py:256: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.\u001b[0m\n",
      "  deprecation(\n",
      "c:\\Users\\simon\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -0.277\n",
      "---------------------------------------\n",
      "Total T: 999 Episode Num: 1 Episode T: 999 Reward: -32.504\n",
      "Total T: 1998 Episode Num: 2 Episode T: 999 Reward: -35.036\n",
      "Total T: 2997 Episode Num: 3 Episode T: 999 Reward: -32.692\n",
      "Total T: 3996 Episode Num: 4 Episode T: 999 Reward: -32.465\n",
      "Total T: 4995 Episode Num: 5 Episode T: 999 Reward: -35.142\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -0.277\n",
      "---------------------------------------\n",
      "Total T: 5994 Episode Num: 6 Episode T: 999 Reward: -32.234\n",
      "Total T: 6993 Episode Num: 7 Episode T: 999 Reward: -34.059\n",
      "Total T: 7992 Episode Num: 8 Episode T: 999 Reward: -33.792\n",
      "Total T: 8991 Episode Num: 9 Episode T: 999 Reward: -33.278\n",
      "Total T: 9990 Episode Num: 10 Episode T: 999 Reward: -32.696\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -0.277\n",
      "---------------------------------------\n",
      "Total T: 10989 Episode Num: 11 Episode T: 999 Reward: -32.580\n",
      "Total T: 11988 Episode Num: 12 Episode T: 999 Reward: -33.955\n",
      "Total T: 12987 Episode Num: 13 Episode T: 999 Reward: -33.182\n",
      "Total T: 13986 Episode Num: 14 Episode T: 999 Reward: -33.186\n",
      "Total T: 14985 Episode Num: 15 Episode T: 999 Reward: -32.310\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -0.277\n",
      "---------------------------------------\n",
      "Total T: 15984 Episode Num: 16 Episode T: 999 Reward: -32.807\n",
      "Total T: 16983 Episode Num: 17 Episode T: 999 Reward: -31.836\n",
      "Total T: 17982 Episode Num: 18 Episode T: 999 Reward: -33.139\n",
      "Total T: 18981 Episode Num: 19 Episode T: 999 Reward: -32.588\n",
      "Total T: 19980 Episode Num: 20 Episode T: 999 Reward: -32.118\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -0.277\n",
      "---------------------------------------\n",
      "Total T: 20979 Episode Num: 21 Episode T: 999 Reward: -32.293\n",
      "Total T: 21978 Episode Num: 22 Episode T: 999 Reward: -32.756\n",
      "Total T: 22977 Episode Num: 23 Episode T: 999 Reward: -32.745\n",
      "Total T: 23976 Episode Num: 24 Episode T: 999 Reward: -31.752\n",
      "Total T: 24975 Episode Num: 25 Episode T: 999 Reward: -33.952\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -0.277\n",
      "---------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 134\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;66;03m# Train agent after collecting sufficient data\u001b[39;00m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m t \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m args\u001b[38;5;241m.\u001b[39mstart_timesteps:\n\u001b[1;32m--> 134\u001b[0m \t\u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplay_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m done: \n\u001b[0;32m    137\u001b[0m \t\u001b[38;5;66;03m# +1 to account for 0 indexing. +0 on ep_timesteps since it will increment +1 even if done=True\u001b[39;00m\n\u001b[0;32m    138\u001b[0m \t\u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal T: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Episode Num: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepisode_num\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Episode T: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepisode_timesteps\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Reward: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepisode_reward\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[3], line 120\u001b[0m, in \u001b[0;36mTD3.train\u001b[1;34m(self, replay_buffer, batch_size)\u001b[0m\n\u001b[0;32m    115\u001b[0m next_action \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    116\u001b[0m \t\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor_target(next_state) \u001b[38;5;241m+\u001b[39m noise\n\u001b[0;32m    117\u001b[0m )\u001b[38;5;241m.\u001b[39mclamp(\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_action, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_action)\n\u001b[0;32m    119\u001b[0m \u001b[38;5;66;03m# Compute the target Q value\u001b[39;00m\n\u001b[1;32m--> 120\u001b[0m target_Q1, target_Q2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcritic_target\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnext_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_action\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    121\u001b[0m target_Q \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmin(target_Q1, target_Q2)\n\u001b[0;32m    122\u001b[0m target_Q \u001b[38;5;241m=\u001b[39m reward \u001b[38;5;241m+\u001b[39m not_done \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdiscount \u001b[38;5;241m*\u001b[39m target_Q\n",
      "File \u001b[1;32mc:\\Users\\simon\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\simon\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[3], line 49\u001b[0m, in \u001b[0;36mCritic.forward\u001b[1;34m(self, state, action)\u001b[0m\n\u001b[0;32m     46\u001b[0m sa \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([state, action], \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     48\u001b[0m q1 \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ml1(sa))\n\u001b[1;32m---> 49\u001b[0m q1 \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43ml2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq1\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     50\u001b[0m q1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ml3(q1)\n\u001b[0;32m     52\u001b[0m q2 \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ml4(sa))\n",
      "File \u001b[1;32mc:\\Users\\simon\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\simon\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\simon\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# MAIN FILE\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Runs policy for X episodes and returns average reward\n",
    "# A fixed seed is used for the eval environment\n",
    "def eval_policy(policy, env_name, seed, eval_episodes=10):\n",
    "\teval_env = gym.make(env_name)\n",
    "\teval_env.seed(seed + 100)\n",
    "\n",
    "\tavg_reward = 0.\n",
    "\tfor _ in range(eval_episodes):\n",
    "\t\tstate, done = eval_env.reset(), False\n",
    "\t\twhile not done:\n",
    "\t\t\taction = policy.select_action(np.array(state))\n",
    "\t\t\tstate, reward, done, _ = eval_env.step(action)\n",
    "\t\t\tavg_reward += reward\n",
    "\n",
    "\tavg_reward /= eval_episodes\n",
    "\n",
    "\tprint(\"---------------------------------------\")\n",
    "\tprint(f\"Evaluation over {eval_episodes} episodes: {avg_reward:.3f}\")\n",
    "\tprint(\"---------------------------------------\")\n",
    "\treturn avg_reward\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\t\n",
    "\t'''\n",
    "\tparser = argparse.ArgumentParser()\n",
    "\tparser.add_argument(\"--policy\", default=\"TD3\")                  # Policy name (TD3, DDPG or OurDDPG)\n",
    "\tparser.add_argument(\"--env\", default=\"HalfCheetah-v2\")          # OpenAI gym environment name\n",
    "\tparser.add_argument(\"--seed\", default=0, type=int)              # Sets Gym, PyTorch and Numpy seeds\n",
    "\tparser.add_argument(\"--start_timesteps\", default=25e3, type=int)# Time steps initial random policy is used\n",
    "\tparser.add_argument(\"--eval_freq\", default=5e3, type=int)       # How often (time steps) we evaluate\n",
    "\tparser.add_argument(\"--max_timesteps\", default=1e6, type=int)   # Max time steps to run environment\n",
    "\tparser.add_argument(\"--expl_noise\", default=0.1, type=float)    # Std of Gaussian exploration noise\n",
    "\tparser.add_argument(\"--batch_size\", default=256, type=int)      # Batch size for both actor and critic\n",
    "\tparser.add_argument(\"--discount\", default=0.99, type=float)     # Discount factor\n",
    "\tparser.add_argument(\"--tau\", default=0.005, type=float)         # Target network update rate\n",
    "\tparser.add_argument(\"--policy_noise\", default=0.2)              # Noise added to target policy during critic update\n",
    "\tparser.add_argument(\"--noise_clip\", default=0.5)                # Range to clip target policy noise\n",
    "\tparser.add_argument(\"--policy_freq\", default=2, type=int)       # Frequency of delayed policy updates\n",
    "\tparser.add_argument(\"--save_model\", action=\"store_true\")        # Save model and optimizer parameters\n",
    "\tparser.add_argument(\"--load_model\", default=\"\")                 # Model load file name, \"\" doesn't load, \"default\" uses file_name\n",
    "\targs = parser.parse_args()\n",
    "\t'''\n",
    "\targs = Args()\n",
    "\n",
    "\tfile_name = f\"{args.policy}_{args.env}_{args.seed}\"\n",
    "\tprint(\"---------------------------------------\")\n",
    "\tprint(f\"Policy: {args.policy}, Env: {args.env}, Seed: {args.seed}\")\n",
    "\tprint(\"---------------------------------------\")\n",
    "\n",
    "\tif not os.path.exists(\"./results\"):\n",
    "\t\tos.makedirs(\"./results\")\n",
    "\n",
    "\tif args.save_model and not os.path.exists(\"./models\"):\n",
    "\t\tos.makedirs(\"./models\")\n",
    "\n",
    "\tenv = gym.make(args.env)\n",
    "\n",
    "\t# Set seeds\n",
    "\tenv.seed(args.seed)\n",
    "\tenv.action_space.seed(args.seed)\n",
    "\ttorch.manual_seed(args.seed)\n",
    "\tnp.random.seed(args.seed)\n",
    "\t\n",
    "\tstate_dim = env.observation_space.shape[0]\n",
    "\taction_dim = env.action_space.shape[0] \n",
    "\tmax_action = float(env.action_space.high[0])\n",
    "\n",
    "\tkwargs = {\n",
    "\t\t\"state_dim\": state_dim,\n",
    "\t\t\"action_dim\": action_dim,\n",
    "\t\t\"max_action\": max_action,\n",
    "\t\t\"discount\": args.discount,\n",
    "\t\t\"tau\": args.tau,\n",
    "\t}\n",
    "\n",
    "\t# Initialize policy\n",
    "\tif args.policy == \"TD3\":\n",
    "\t\t# Target policy smoothing is scaled wrt the action scale\n",
    "\t\tkwargs[\"policy_noise\"] = args.policy_noise * max_action\n",
    "\t\tkwargs[\"noise_clip\"] = args.noise_clip * max_action\n",
    "\t\tkwargs[\"policy_freq\"] = args.policy_freq\n",
    "\t\t#policy = TD3.TD3(**kwargs)\n",
    "\t\tpolicy = TD3(**kwargs)\n",
    "\telif args.policy == \"OurDDPG\":\n",
    "\t\tpolicy = OurDDPG.DDPG(**kwargs)\n",
    "\telif args.policy == \"DDPG\":\n",
    "\t\tpolicy = DDPG.DDPG(**kwargs)\n",
    "\n",
    "\tif args.load_model != \"\":\n",
    "\t\tpolicy_file = file_name if args.load_model == \"default\" else args.load_model\n",
    "\t\tpolicy.load(f\"./models/{policy_file}\")\n",
    "\n",
    "\treplay_buffer = utils.ReplayBuffer(state_dim, action_dim)\n",
    "\t\n",
    "\t# Evaluate untrained policy\n",
    "\tevaluations = [eval_policy(policy, args.env, args.seed)]\n",
    "\n",
    "\tstate, done = env.reset(), False\n",
    "\tepisode_reward = 0\n",
    "\tepisode_timesteps = 0\n",
    "\tepisode_num = 0\n",
    "\n",
    "\tfor t in range(int(args.max_timesteps)):\n",
    "\t\t\n",
    "\t\tepisode_timesteps += 1\n",
    "\n",
    "\t\t# Select action randomly or according to policy\n",
    "\t\tif t < args.start_timesteps:\n",
    "\t\t\taction = env.action_space.sample()\n",
    "\t\telse:\n",
    "\t\t\taction = (\n",
    "\t\t\t\tpolicy.select_action(np.array(state))\n",
    "\t\t\t\t+ np.random.normal(0, max_action * args.expl_noise, size=action_dim)\n",
    "\t\t\t).clip(-max_action, max_action)\n",
    "\n",
    "\t\t# Perform action\n",
    "\t\tnext_state, reward, done, _ = env.step(action) \n",
    "\t\tdone_bool = float(done) if episode_timesteps < env._max_episode_steps else 0\n",
    "\n",
    "\t\t# Store data in replay buffer\n",
    "\t\treplay_buffer.add(state, action, next_state, reward, done_bool)\n",
    "\n",
    "\t\tstate = next_state\n",
    "\t\tepisode_reward += reward\n",
    "\n",
    "\t\t# Train agent after collecting sufficient data\n",
    "\t\tif t >= args.start_timesteps:\n",
    "\t\t\tpolicy.train(replay_buffer, args.batch_size)\n",
    "\n",
    "\t\tif done: \n",
    "\t\t\t# +1 to account for 0 indexing. +0 on ep_timesteps since it will increment +1 even if done=True\n",
    "\t\t\tprint(f\"Total T: {t+1} Episode Num: {episode_num+1} Episode T: {episode_timesteps} Reward: {episode_reward:.3f}\")\n",
    "\t\t\t# Reset environment\n",
    "\t\t\tstate, done = env.reset(), False\n",
    "\t\t\tepisode_reward = 0\n",
    "\t\t\tepisode_timesteps = 0\n",
    "\t\t\tepisode_num += 1 \n",
    "\n",
    "\t\t# Evaluate episode\n",
    "\t\tif (t + 1) % args.eval_freq == 0:\n",
    "\t\t\tevaluations.append(eval_policy(policy, args.env, args.seed))\n",
    "\t\t\tnp.save(f\"./results/{file_name}\", evaluations)\n",
    "\t\t\tif args.save_model: policy.save(f\"./models/{file_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<utils.ReplayBuffer at 0x253c1282b10>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replay_buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'act' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 22\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m ep_count \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m50\u001b[39m:\n\u001b[0;32m     21\u001b[0m     env\u001b[38;5;241m.\u001b[39mrender()\n\u001b[1;32m---> 22\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[43mact\u001b[49m(state, \u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# Ensure this function now returns a continuous action\u001b[39;00m\n\u001b[0;32m     23\u001b[0m     state, reward, terminated, truncated, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep([action\u001b[38;5;241m.\u001b[39mitem()])  \u001b[38;5;66;03m# Execute action, note the list around action.item()\u001b[39;00m\n\u001b[0;32m     25\u001b[0m     state \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(state, device\u001b[38;5;241m=\u001b[39mdevice)\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'act' is not defined"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Visualization\n",
    "\n",
    "'''\n",
    "# Assuming 'device' and 'act' are defined elsewhere in your script\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize the continuous version of the Mountain Car environment\n",
    "env = gym.make(\"MountainCarContinuous-v0\", render_mode=\"human\")\n",
    "\n",
    "# Reset the environment and prepare the initial state\n",
    "state, _ = env.reset()\n",
    "state = torch.tensor(state, device=device).float().unsqueeze(0)\n",
    "\n",
    "# Initialize variables to track episodes and scores\n",
    "ep_count = 0\n",
    "current_ep_reward = 0\n",
    "scores = []\n",
    "\n",
    "# Run simulation for 50 episodes\n",
    "while ep_count < 50:\n",
    "    env.render()\n",
    "    action = act(state, 0)  # Ensure this function now returns a continuous action\n",
    "    state, reward, terminated, truncated, _ = env.step([action.item()])  # Execute action, note the list around action.item()\n",
    "\n",
    "    state = torch.tensor(state, device=device).float().unsqueeze(0)\n",
    "    current_ep_reward += reward\n",
    "    done = terminated or truncated\n",
    "    \n",
    "    if done:\n",
    "        ep_count += 1\n",
    "        scores.append(current_ep_reward)\n",
    "        current_ep_reward = 0\n",
    "        state, _ = env.reset()  # Reset for the next episode\n",
    "        state = torch.tensor(state, device=device).float().unsqueeze(0)\n",
    "\n",
    "env.close()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
