{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import random\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Replay Memory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',('state', 'action', 'next_state', 'reward', 'done'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = deque([], maxlen = self.capacity) # cyclic buffer of bounded size\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "    \n",
    "    def push(self, *args):\n",
    "        self.memory.append(Transition(*args))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DQN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, n_observations, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.layers = torch.nn.Sequential(\n",
    "            torch.nn.Linear(n_observations, 256),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(256,256), # only one hidden layer, the first two regarding the convolution as processing of input images are dropped\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(256, n_actions)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Environment***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"MountainCar-v0\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.99\n",
    "EPS_START = 0.99\n",
    "EPS_END = 0.001\n",
    "EPS_DECAY = 0.9\n",
    "REPLAY_MEMORY = 100000\n",
    "LR = 0.0005 #1e-4\n",
    "\n",
    "\n",
    "state, info = env.reset()\n",
    "n_actions = env.action_space.n\n",
    "n_observations = len(state)\n",
    "\n",
    "memory = ReplayMemory(REPLAY_MEMORY)\n",
    "\n",
    "estimate_network = DQN(n_observations, n_actions)\n",
    "target_network = DQN(n_observations, n_actions)\n",
    "target_network.load_state_dict(estimate_network.state_dict())\n",
    "\n",
    "optimizer = torch.optim.AdamW(estimate_network.parameters(), lr = LR)\n",
    "\n",
    "episode_durations = []\n",
    "\n",
    "def act(state, eps):\n",
    "    if random.random() < eps:\n",
    "        return torch.tensor([[env.action_space.sample()]], device=device, dtype=torch.long)\n",
    "    else: \n",
    "        with torch.no_grad(): # improves computation\n",
    "            return estimate_network(state).max(1).indices.view(1,1)\n",
    "        \n",
    "def plot_durations(show_result=False):\n",
    "    plt.figure(1)\n",
    "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
    "    if show_result:\n",
    "        plt.title('Result')\n",
    "    else:\n",
    "        plt.clf()\n",
    "        plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(durations_t.numpy())\n",
    "    # Take 100 episode averages and plot them too\n",
    "    if len(durations_t) >= 100:\n",
    "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy())\n",
    "\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "    if is_ipython:\n",
    "        if not show_result:\n",
    "            display.display(plt.gcf())\n",
    "            display.clear_output(wait=True)\n",
    "        else:\n",
    "            display.display(plt.gcf())\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HanaCatic\\AppData\\Local\\Temp\\ipykernel_27288\\2909915724.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  action = torch.tensor(action).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 0 : -200.0 average score\n",
      "episode 25 : -200.0 average score\n",
      "episode 50 : -200.0 average score\n",
      "episode 75 : -200.0 average score\n",
      "episode 100 : -200.0 average score\n",
      "episode 125 : -200.0 average score\n",
      "episode 150 : -200.0 average score\n",
      "episode 175 : -200.0 average score\n",
      "episode 200 : -200.0 average score\n",
      "episode 225 : -200.0 average score\n",
      "episode 250 : -200.0 average score\n",
      "episode 275 : -197.96 average score\n",
      "episode 300 : -161.84 average score\n",
      "episode 325 : -142.92 average score\n",
      "episode 350 : -142.16 average score\n",
      "episode 375 : -141.8 average score\n",
      "episode 400 : -137.2 average score\n",
      "episode 425 : -148.0 average score\n",
      "episode 450 : -140.6 average score\n",
      "episode 475 : -139.76 average score\n",
      "episode 500 : -124.96 average score\n",
      "episode 525 : -119.6 average score\n",
      "episode 550 : -135.88 average score\n",
      "episode 575 : -143.68 average score\n",
      "episode 600 : -137.68 average score\n",
      "episode 625 : -139.24 average score\n",
      "episode 650 : -130.72 average score\n",
      "episode 675 : -136.8 average score\n",
      "episode 700 : -142.4 average score\n",
      "episode 725 : -126.08 average score\n",
      "episode 750 : -130.8 average score\n",
      "episode 775 : -121.0 average score\n",
      "episode 800 : -114.52 average score\n",
      "episode 825 : -109.56 average score\n",
      "episode 850 : -99.08 average score\n",
      "episode 875 : -105.32 average score\n",
      "episode 900 : -109.04 average score\n",
      "episode 925 : -109.76 average score\n",
      "episode 950 : -111.68 average score\n",
      "episode 975 : -111.6 average score\n"
     ]
    }
   ],
   "source": [
    "episode_rewards = []\n",
    "successful_sequences = 0\n",
    "eps = EPS_START\n",
    "for ep in range(1000):\n",
    "    state, info = env.reset() # info in some environments has some additional information\n",
    "    state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "    \n",
    "    while not done:\n",
    "        action = act(state, eps) # epsilon greedy \n",
    "        action = torch.tensor(action).to(device)\n",
    "        \n",
    "        next_state, reward, terminated, truncated, _  = env.step(action.item()) # execute action\n",
    "        done = terminated or truncated\n",
    "        episode_reward += reward\n",
    "        \n",
    "        \n",
    "        next_state = torch.tensor(next_state, dtype=torch.float32, device=device).unsqueeze(0) # get next state\n",
    "        reward = torch.tensor(reward).to(device).float().unsqueeze(0) \n",
    "        done = torch.tensor(done).to(device).unsqueeze(0)\n",
    "\n",
    "        memory.push(state, action, next_state, reward, done) # store transition in replay memory\n",
    "        \n",
    "        state = next_state\n",
    "        \n",
    "        if len(memory) > BATCH_SIZE: \n",
    "            transitions = memory.sample(BATCH_SIZE) # sample batch from replay memory\n",
    "            batch = Transition(*zip(*transitions))\n",
    "\n",
    "            next_state_batch = torch.cat(batch.next_state)\n",
    "            state_batch = torch.cat(batch.state)\n",
    "            action_batch = torch.cat(batch.action)\n",
    "            reward_batch = torch.cat(batch.reward)\n",
    "            done_batch = torch.cat(batch.done)\n",
    "            \n",
    "            # determine if next state is terminal state (two approaches, one add done as part of the transition to the replay memory and second to try to determine it)\n",
    "            # non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), device=device, dtype=torch.bool)\n",
    "            # but this will only check if it s terminal in the batch and not overall the influnce of this would be interesting to check for sure\n",
    "            \n",
    "            y = reward_batch + (done_batch == False) * GAMMA * target_network(next_state_batch).max(1).values # compute target\n",
    "            \n",
    "            criterion = torch.nn.SmoothL1Loss() # criterion defined as Huber loss less sensitive to outliers\n",
    "            loss = criterion(y, estimate_network(state_batch).gather(1, action_batch).squeeze(1)) # compute the loss\n",
    "            \n",
    "            optimizer.zero_grad() # not to accumulate gradients\n",
    "            loss.backward() # backpropagate the loss\n",
    "            for params in estimate_network.parameters():\n",
    "                params.grad.data.clamp_(-1,1)\n",
    "            # torch.nn.utils.clip_grad(estimate_network.parameters(), 1)  # why is clipping the parameters important do they get out of hand?       \n",
    "            optimizer.step()\n",
    "            # updating the target network\n",
    "            # one approach soft update and second approach delayed update to estimated network parameters\n",
    "            \n",
    "    if ep % 5 == 0:\n",
    "        target_network.load_state_dict(estimate_network.state_dict())\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "    eps = max(EPS_END, eps*EPS_DECAY) # decay epsilon\n",
    "    episode_rewards.append(episode_reward) \n",
    "    \n",
    "\n",
    "    if ep % 25 == 0: \n",
    "        print('episode', ep, ':', np.mean(episode_rewards[-25:]), 'average score')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[46], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m scores \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m ep_count \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m50\u001b[39m:\n\u001b[1;32m---> 10\u001b[0m     \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m     action \u001b[38;5;241m=\u001b[39m act(state, \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     12\u001b[0m     state, reward, terminated, truncated, _  \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action\u001b[38;5;241m.\u001b[39mitem()) \u001b[38;5;66;03m# execute action\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\HanaCatic\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gymnasium\\core.py:471\u001b[0m, in \u001b[0;36mWrapper.render\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    469\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrender\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m RenderFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mlist\u001b[39m[RenderFrame] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    470\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Uses the :meth:`render` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 471\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\HanaCatic\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gymnasium\\wrappers\\order_enforcing.py:70\u001b[0m, in \u001b[0;36mOrderEnforcing.render\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_disable_render_order_enforcing \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\n\u001b[0;32m     67\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call `env.render()` before calling `env.reset()`, if this is a intended action, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     68\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset `disable_render_order_enforcing=True` on the OrderEnforcer wrapper.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     69\u001b[0m     )\n\u001b[1;32m---> 70\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\HanaCatic\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gymnasium\\wrappers\\env_checker.py:67\u001b[0m, in \u001b[0;36mPassiveEnvChecker.render\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_render_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 67\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\HanaCatic\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gymnasium\\envs\\classic_control\\mountain_car.py:266\u001b[0m, in \u001b[0;36mMountainCarEnv.render\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    265\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mevent\u001b[38;5;241m.\u001b[39mpump()\n\u001b[1;32m--> 266\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrender_fps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    267\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mdisplay\u001b[38;5;241m.\u001b[39mflip()\n\u001b[0;32m    269\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrgb_array\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env.close()\n",
    "env = gym.make(\"MountainCar-v0\", render_mode=\"human\")\n",
    "\n",
    "state, _ = env.reset()\n",
    "\n",
    "state = torch.tensor(state).to(device).float().unsqueeze(0)\n",
    "ep_count = 0\n",
    "current_ep_reward = 0\n",
    "scores = []\n",
    "while ep_count < 50:\n",
    "    env.render()\n",
    "    action = act(state, 0)\n",
    "    state, reward, terminated, truncated, _  = env.step(action.item()) # execute action\n",
    "    state = torch.tensor(state).to(device).float().unsqueeze(0)\n",
    "    current_ep_reward += reward\n",
    "    done = terminated or truncated \n",
    "    \n",
    "    if done:\n",
    "        ep_count += 1\n",
    "        scores.append(current_ep_reward)\n",
    "        current_ep_reward = 0\n",
    "        state, _ = env.reset()\n",
    "        state = torch.tensor(state).to(device).float().unsqueeze(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-114.0, -115.0, -116.0, -113.0, -115.0, -118.0, -112.0, -114.0, -116.0, -116.0, -121.0, -114.0, -118.0, -114.0, -120.0, -113.0, -112.0, -114.0, -115.0, -116.0, -113.0, -120.0, -113.0, -112.0, -114.0, -114.0, -112.0, -112.0, -114.0, -112.0, -114.0, -114.0, -112.0, -116.0, -120.0, -112.0, -113.0, -116.0, -116.0, -114.0, -114.0, -113.0, -116.0, -112.0, -112.0, -117.0, -113.0, -116.0, -112.0, -121.0]\n"
     ]
    }
   ],
   "source": [
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
