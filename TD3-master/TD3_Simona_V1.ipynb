{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TD3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORTS\n",
    "\n",
    "# TD3\n",
    "import copy\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Main\n",
    "import numpy as np\n",
    "import torch\n",
    "import gym\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "import utils\n",
    "#import TD3\n",
    "import OurDDPG\n",
    "import DDPG\n",
    "\n",
    "# Visualization\n",
    "import gym\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TD3 FILE\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Implementation of Twin Delayed Deep Deterministic Policy Gradients (TD3)\n",
    "# Paper: https://arxiv.org/abs/1802.09477\n",
    "\n",
    "\n",
    "class Actor(nn.Module):\n",
    "\tdef __init__(self, state_dim, action_dim, max_action):\n",
    "\t\tsuper(Actor, self).__init__()\n",
    "\n",
    "\t\tself.l1 = nn.Linear(state_dim, 256)\n",
    "\t\tself.l2 = nn.Linear(256, 256)\n",
    "\t\tself.l3 = nn.Linear(256, action_dim)\n",
    "\t\t\n",
    "\t\tself.max_action = max_action\n",
    "\t\t\n",
    "\n",
    "\tdef forward(self, state):\n",
    "\t\ta = F.relu(self.l1(state))\n",
    "\t\ta = F.relu(self.l2(a))\n",
    "\t\treturn self.max_action * torch.tanh(self.l3(a))\n",
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "\tdef __init__(self, state_dim, action_dim):\n",
    "\t\tsuper(Critic, self).__init__()\n",
    "\n",
    "\t\t# Q1 architecture\n",
    "\t\tself.l1 = nn.Linear(state_dim + action_dim, 256)\n",
    "\t\tself.l2 = nn.Linear(256, 256)\n",
    "\t\tself.l3 = nn.Linear(256, 1)\n",
    "\n",
    "\t\t# Q2 architecture\n",
    "\t\tself.l4 = nn.Linear(state_dim + action_dim, 256)\n",
    "\t\tself.l5 = nn.Linear(256, 256)\n",
    "\t\tself.l6 = nn.Linear(256, 1)\n",
    "\n",
    "\n",
    "\tdef forward(self, state, action):\n",
    "\t\tsa = torch.cat([state, action], 1)\n",
    "\n",
    "\t\tq1 = F.relu(self.l1(sa))\n",
    "\t\tq1 = F.relu(self.l2(q1))\n",
    "\t\tq1 = self.l3(q1)\n",
    "\n",
    "\t\tq2 = F.relu(self.l4(sa))\n",
    "\t\tq2 = F.relu(self.l5(q2))\n",
    "\t\tq2 = self.l6(q2)\n",
    "\t\treturn q1, q2\n",
    "\n",
    "\n",
    "\tdef Q1(self, state, action):\n",
    "\t\tsa = torch.cat([state, action], 1)\n",
    "\n",
    "\t\tq1 = F.relu(self.l1(sa))\n",
    "\t\tq1 = F.relu(self.l2(q1))\n",
    "\t\tq1 = self.l3(q1)\n",
    "\t\treturn q1\n",
    "\n",
    "\n",
    "class TD3(object):\n",
    "\tdef __init__(\n",
    "\t\tself,\n",
    "\t\tstate_dim,\n",
    "\t\taction_dim,\n",
    "\t\tmax_action,\n",
    "\t\tdiscount=0.99,\n",
    "\t\ttau=0.005,\n",
    "\t\tpolicy_noise=0.2,\n",
    "\t\tnoise_clip=0.5,\n",
    "\t\tpolicy_freq=2\n",
    "\t):\n",
    "\n",
    "\t\tself.actor = Actor(state_dim, action_dim, max_action).to(device)\n",
    "\t\tself.actor_target = copy.deepcopy(self.actor)\n",
    "\t\tself.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=3e-4)\n",
    "\n",
    "\t\tself.critic = Critic(state_dim, action_dim).to(device)\n",
    "\t\tself.critic_target = copy.deepcopy(self.critic)\n",
    "\t\tself.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=3e-4)\n",
    "\n",
    "\t\tself.max_action = max_action\n",
    "\t\tself.discount = discount\n",
    "\t\tself.tau = tau\n",
    "\t\tself.policy_noise = policy_noise\n",
    "\t\tself.noise_clip = noise_clip\n",
    "\t\tself.policy_freq = policy_freq\n",
    "\n",
    "\t\tself.total_it = 0\n",
    "\n",
    "\n",
    "\tdef select_action(self, state):\n",
    "\t\tstate = torch.FloatTensor(state.reshape(1, -1)).to(device)\n",
    "\t\treturn self.actor(state).cpu().data.numpy().flatten()\n",
    "\n",
    "\n",
    "\tdef train(self, replay_buffer, batch_size=256):\n",
    "\t\tself.total_it += 1\n",
    "\n",
    "\t\t# Sample replay buffer \n",
    "\t\tstate, action, next_state, reward, not_done = replay_buffer.sample(batch_size)\n",
    "\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\t# Select action according to policy and add clipped noise\n",
    "\t\t\tnoise = (\n",
    "\t\t\t\ttorch.randn_like(action) * self.policy_noise\n",
    "\t\t\t).clamp(-self.noise_clip, self.noise_clip)\n",
    "\t\t\t\n",
    "\t\t\tnext_action = (\n",
    "\t\t\t\tself.actor_target(next_state) + noise\n",
    "\t\t\t).clamp(-self.max_action, self.max_action)\n",
    "\n",
    "\t\t\t# Compute the target Q value\n",
    "\t\t\ttarget_Q1, target_Q2 = self.critic_target(next_state, next_action)\n",
    "\t\t\ttarget_Q = torch.min(target_Q1, target_Q2)\n",
    "\t\t\ttarget_Q = reward + not_done * self.discount * target_Q\n",
    "\n",
    "\t\t# Get current Q estimates\n",
    "\t\tcurrent_Q1, current_Q2 = self.critic(state, action)\n",
    "\n",
    "\t\t# Compute critic loss\n",
    "\t\tcritic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q)\n",
    "\n",
    "\t\t# Optimize the critic\n",
    "\t\tself.critic_optimizer.zero_grad()\n",
    "\t\tcritic_loss.backward()\n",
    "\t\tself.critic_optimizer.step()\n",
    "\n",
    "\t\t# Delayed policy updates\n",
    "\t\tif self.total_it % self.policy_freq == 0:\n",
    "\n",
    "\t\t\t# Compute actor losse\n",
    "\t\t\tactor_loss = -self.critic.Q1(state, self.actor(state)).mean()\n",
    "\t\t\t\n",
    "\t\t\t# Optimize the actor \n",
    "\t\t\tself.actor_optimizer.zero_grad()\n",
    "\t\t\tactor_loss.backward()\n",
    "\t\t\tself.actor_optimizer.step()\n",
    "\n",
    "\t\t\t# Update the frozen target models\n",
    "\t\t\tfor param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
    "\t\t\t\ttarget_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "\n",
    "\t\t\tfor param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
    "\t\t\t\ttarget_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "\n",
    "\n",
    "\tdef save(self, filename):\n",
    "\t\ttorch.save(self.critic.state_dict(), filename + \"_critic\")\n",
    "\t\ttorch.save(self.critic_optimizer.state_dict(), filename + \"_critic_optimizer\")\n",
    "\t\t\n",
    "\t\ttorch.save(self.actor.state_dict(), filename + \"_actor\")\n",
    "\t\ttorch.save(self.actor_optimizer.state_dict(), filename + \"_actor_optimizer\")\n",
    "\n",
    "\n",
    "\tdef load(self, filename):\n",
    "\t\tself.critic.load_state_dict(torch.load(filename + \"_critic\"))\n",
    "\t\tself.critic_optimizer.load_state_dict(torch.load(filename + \"_critic_optimizer\"))\n",
    "\t\tself.critic_target = copy.deepcopy(self.critic)\n",
    "\n",
    "\t\tself.actor.load_state_dict(torch.load(filename + \"_actor\"))\n",
    "\t\tself.actor_optimizer.load_state_dict(torch.load(filename + \"_actor_optimizer\"))\n",
    "\t\tself.actor_target = copy.deepcopy(self.actor)\n",
    "\t\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters settings\n",
    "\n",
    "class Args:\n",
    "    policy = \"TD3\"                  # Policy name (TD3, DDPG, or OurDDPG)\n",
    "    env = \"MountainCarContinuous-v0\"          # OpenAI gym environment name\n",
    "    seed = 0                        # Sets Gym, PyTorch, and Numpy seeds\n",
    "    start_timesteps = 25000         # Time steps initial random policy is used\n",
    "    eval_freq = 5000                # How often (time steps) we evaluate\n",
    "    max_timesteps = 150000         # Max time steps to run environment\n",
    "    expl_noise = 2               # Std of Gaussian exploration noise\n",
    "    batch_size = 256                # Batch size for both actor and critic\n",
    "    discount = 0.99                 # Discount factor\n",
    "    tau = 0.05                     # Target network update rate\n",
    "    policy_noise = 0.2              # Noise added to target policy during critic update\n",
    "    noise_clip = 0.5                # Range to clip target policy noise\n",
    "    policy_freq = 2                 # Frequency of delayed policy updates\n",
    "    save_model = True              # Save model and optimizer parameters\n",
    "    load_model = \"\"                 # Model load file name, \"\" doesn't load, \"default\" uses file_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "Policy: TD3, Env: MountainCarContinuous-v0, Seed: 0\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\simon\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gym\\core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "c:\\Users\\simon\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gym\\wrappers\\step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "c:\\Users\\simon\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gym\\core.py:256: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.\u001b[0m\n",
      "  deprecation(\n",
      "c:\\Users\\simon\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gym\\core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "c:\\Users\\simon\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gym\\wrappers\\step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "c:\\Users\\simon\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gym\\core.py:256: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.\u001b[0m\n",
      "  deprecation(\n",
      "c:\\Users\\simon\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -0.277\n",
      "---------------------------------------\n",
      "Total T: 999 Episode Num: 1 Episode T: 999 Reward: -32.504\n",
      "Total T: 1998 Episode Num: 2 Episode T: 999 Reward: -35.036\n",
      "Total T: 2997 Episode Num: 3 Episode T: 999 Reward: -32.692\n",
      "Total T: 3996 Episode Num: 4 Episode T: 999 Reward: -32.465\n",
      "Total T: 4995 Episode Num: 5 Episode T: 999 Reward: -35.142\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -0.277\n",
      "---------------------------------------\n",
      "Total T: 5994 Episode Num: 6 Episode T: 999 Reward: -32.234\n",
      "Total T: 6993 Episode Num: 7 Episode T: 999 Reward: -34.059\n",
      "Total T: 7992 Episode Num: 8 Episode T: 999 Reward: -33.792\n",
      "Total T: 8991 Episode Num: 9 Episode T: 999 Reward: -33.278\n",
      "Total T: 9990 Episode Num: 10 Episode T: 999 Reward: -32.696\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -0.277\n",
      "---------------------------------------\n",
      "Total T: 10989 Episode Num: 11 Episode T: 999 Reward: -32.580\n",
      "Total T: 11988 Episode Num: 12 Episode T: 999 Reward: -33.955\n",
      "Total T: 12987 Episode Num: 13 Episode T: 999 Reward: -33.182\n",
      "Total T: 13986 Episode Num: 14 Episode T: 999 Reward: -33.186\n",
      "Total T: 14985 Episode Num: 15 Episode T: 999 Reward: -32.310\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -0.277\n",
      "---------------------------------------\n",
      "Total T: 15984 Episode Num: 16 Episode T: 999 Reward: -32.807\n",
      "Total T: 16983 Episode Num: 17 Episode T: 999 Reward: -31.836\n",
      "Total T: 17982 Episode Num: 18 Episode T: 999 Reward: -33.139\n",
      "Total T: 18981 Episode Num: 19 Episode T: 999 Reward: -32.588\n",
      "Total T: 19980 Episode Num: 20 Episode T: 999 Reward: -32.118\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -0.277\n",
      "---------------------------------------\n",
      "Total T: 20979 Episode Num: 21 Episode T: 999 Reward: -32.293\n",
      "Total T: 21978 Episode Num: 22 Episode T: 999 Reward: -32.756\n",
      "Total T: 22977 Episode Num: 23 Episode T: 999 Reward: -32.745\n",
      "Total T: 23976 Episode Num: 24 Episode T: 999 Reward: -31.752\n",
      "Total T: 24975 Episode Num: 25 Episode T: 999 Reward: -33.952\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -0.277\n",
      "---------------------------------------\n",
      "Total T: 25974 Episode Num: 26 Episode T: 999 Reward: -72.201\n",
      "Total T: 26973 Episode Num: 27 Episode T: 999 Reward: -72.319\n",
      "Total T: 27972 Episode Num: 28 Episode T: 999 Reward: -73.851\n",
      "Total T: 28971 Episode Num: 29 Episode T: 999 Reward: -72.838\n",
      "Total T: 29970 Episode Num: 30 Episode T: 999 Reward: -72.841\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -0.045\n",
      "---------------------------------------\n",
      "Total T: 30969 Episode Num: 31 Episode T: 999 Reward: -72.515\n",
      "Total T: 31968 Episode Num: 32 Episode T: 999 Reward: -73.354\n",
      "Total T: 32967 Episode Num: 33 Episode T: 999 Reward: -75.024\n",
      "Total T: 33966 Episode Num: 34 Episode T: 999 Reward: -74.987\n",
      "Total T: 34965 Episode Num: 35 Episode T: 999 Reward: -72.926\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -0.078\n",
      "---------------------------------------\n",
      "Total T: 35964 Episode Num: 36 Episode T: 999 Reward: -75.555\n",
      "Total T: 36963 Episode Num: 37 Episode T: 999 Reward: -73.911\n",
      "Total T: 37962 Episode Num: 38 Episode T: 999 Reward: -72.982\n",
      "Total T: 38961 Episode Num: 39 Episode T: 999 Reward: -73.822\n",
      "Total T: 39960 Episode Num: 40 Episode T: 999 Reward: -71.619\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -0.013\n",
      "---------------------------------------\n",
      "Total T: 40959 Episode Num: 41 Episode T: 999 Reward: -76.141\n",
      "Total T: 41958 Episode Num: 42 Episode T: 999 Reward: -75.659\n",
      "Total T: 42957 Episode Num: 43 Episode T: 999 Reward: -73.385\n",
      "Total T: 43956 Episode Num: 44 Episode T: 999 Reward: -73.750\n",
      "Total T: 44955 Episode Num: 45 Episode T: 999 Reward: -72.541\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -0.046\n",
      "---------------------------------------\n",
      "Total T: 45954 Episode Num: 46 Episode T: 999 Reward: -75.372\n",
      "Total T: 46953 Episode Num: 47 Episode T: 999 Reward: -73.010\n",
      "Total T: 47952 Episode Num: 48 Episode T: 999 Reward: -75.557\n",
      "Total T: 48951 Episode Num: 49 Episode T: 999 Reward: -74.729\n",
      "Total T: 49950 Episode Num: 50 Episode T: 999 Reward: -73.345\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -0.026\n",
      "---------------------------------------\n",
      "Total T: 50949 Episode Num: 51 Episode T: 999 Reward: -74.811\n",
      "Total T: 51948 Episode Num: 52 Episode T: 999 Reward: -74.562\n",
      "Total T: 52675 Episode Num: 53 Episode T: 727 Reward: 45.119\n",
      "Total T: 53674 Episode Num: 54 Episode T: 999 Reward: -72.800\n",
      "Total T: 54673 Episode Num: 55 Episode T: 999 Reward: -74.759\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -0.105\n",
      "---------------------------------------\n",
      "Total T: 55672 Episode Num: 56 Episode T: 999 Reward: -71.940\n",
      "Total T: 56671 Episode Num: 57 Episode T: 999 Reward: -73.546\n",
      "Total T: 57546 Episode Num: 58 Episode T: 875 Reward: 34.751\n",
      "Total T: 58124 Episode Num: 59 Episode T: 578 Reward: 57.258\n",
      "Total T: 59095 Episode Num: 60 Episode T: 971 Reward: 27.035\n",
      "Total T: 59545 Episode Num: 61 Episode T: 450 Reward: 64.938\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: 46.089\n",
      "---------------------------------------\n",
      "Total T: 60478 Episode Num: 62 Episode T: 933 Reward: 32.043\n",
      "Total T: 61060 Episode Num: 63 Episode T: 582 Reward: 56.546\n",
      "Total T: 61671 Episode Num: 64 Episode T: 611 Reward: 53.922\n",
      "Total T: 61986 Episode Num: 65 Episode T: 315 Reward: 76.258\n",
      "Total T: 62985 Episode Num: 66 Episode T: 999 Reward: -73.969\n",
      "Total T: 63545 Episode Num: 67 Episode T: 560 Reward: 57.363\n",
      "Total T: 64276 Episode Num: 68 Episode T: 731 Reward: 46.061\n",
      "Total T: 64573 Episode Num: 69 Episode T: 297 Reward: 77.035\n",
      "Total T: 64983 Episode Num: 70 Episode T: 410 Reward: 67.902\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: 91.224\n",
      "---------------------------------------\n",
      "Total T: 65420 Episode Num: 71 Episode T: 437 Reward: 66.649\n",
      "Total T: 65585 Episode Num: 72 Episode T: 165 Reward: 87.545\n",
      "Total T: 66304 Episode Num: 73 Episode T: 719 Reward: 46.570\n",
      "Total T: 67015 Episode Num: 74 Episode T: 711 Reward: 44.384\n",
      "Total T: 67657 Episode Num: 75 Episode T: 642 Reward: 52.443\n",
      "Total T: 68149 Episode Num: 76 Episode T: 492 Reward: 61.792\n",
      "Total T: 69007 Episode Num: 77 Episode T: 858 Reward: 34.514\n",
      "Total T: 69418 Episode Num: 78 Episode T: 411 Reward: 69.613\n",
      "Total T: 69867 Episode Num: 79 Episode T: 449 Reward: 67.158\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: 87.726\n",
      "---------------------------------------\n",
      "Total T: 70258 Episode Num: 80 Episode T: 391 Reward: 69.914\n",
      "Total T: 70493 Episode Num: 81 Episode T: 235 Reward: 81.390\n",
      "Total T: 70729 Episode Num: 82 Episode T: 236 Reward: 82.767\n",
      "Total T: 70897 Episode Num: 83 Episode T: 168 Reward: 86.689\n",
      "Total T: 71199 Episode Num: 84 Episode T: 302 Reward: 76.694\n",
      "Total T: 71426 Episode Num: 85 Episode T: 227 Reward: 82.917\n",
      "Total T: 71644 Episode Num: 86 Episode T: 218 Reward: 83.532\n",
      "Total T: 71801 Episode Num: 87 Episode T: 157 Reward: 88.622\n",
      "Total T: 71949 Episode Num: 88 Episode T: 148 Reward: 88.530\n",
      "Total T: 72185 Episode Num: 89 Episode T: 236 Reward: 82.078\n",
      "Total T: 72514 Episode Num: 90 Episode T: 329 Reward: 75.263\n",
      "Total T: 72667 Episode Num: 91 Episode T: 153 Reward: 87.516\n",
      "Total T: 72904 Episode Num: 92 Episode T: 237 Reward: 82.472\n",
      "Total T: 73061 Episode Num: 93 Episode T: 157 Reward: 87.101\n",
      "Total T: 73218 Episode Num: 94 Episode T: 157 Reward: 87.582\n",
      "Total T: 73304 Episode Num: 95 Episode T: 86 Reward: 93.256\n",
      "Total T: 73543 Episode Num: 96 Episode T: 239 Reward: 81.209\n",
      "Total T: 73820 Episode Num: 97 Episode T: 277 Reward: 79.389\n",
      "Total T: 73987 Episode Num: 98 Episode T: 167 Reward: 87.927\n",
      "Total T: 74176 Episode Num: 99 Episode T: 189 Reward: 85.529\n",
      "Total T: 74321 Episode Num: 100 Episode T: 145 Reward: 88.522\n",
      "Total T: 74527 Episode Num: 101 Episode T: 206 Reward: 84.866\n",
      "Total T: 74702 Episode Num: 102 Episode T: 175 Reward: 86.815\n",
      "Total T: 74861 Episode Num: 103 Episode T: 159 Reward: 87.390\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: 92.812\n",
      "---------------------------------------\n",
      "Total T: 75012 Episode Num: 104 Episode T: 151 Reward: 88.128\n",
      "Total T: 75182 Episode Num: 105 Episode T: 170 Reward: 87.346\n",
      "Total T: 75403 Episode Num: 106 Episode T: 221 Reward: 83.274\n",
      "Total T: 75605 Episode Num: 107 Episode T: 202 Reward: 83.959\n",
      "Total T: 75839 Episode Num: 108 Episode T: 234 Reward: 81.491\n",
      "Total T: 76043 Episode Num: 109 Episode T: 204 Reward: 84.373\n",
      "Total T: 76210 Episode Num: 110 Episode T: 167 Reward: 87.495\n",
      "Total T: 76547 Episode Num: 111 Episode T: 337 Reward: 73.729\n",
      "Total T: 76731 Episode Num: 112 Episode T: 184 Reward: 85.917\n",
      "Total T: 76945 Episode Num: 113 Episode T: 214 Reward: 83.849\n",
      "Total T: 77156 Episode Num: 114 Episode T: 211 Reward: 83.278\n",
      "Total T: 77308 Episode Num: 115 Episode T: 152 Reward: 89.013\n",
      "Total T: 77542 Episode Num: 116 Episode T: 234 Reward: 81.443\n",
      "Total T: 77772 Episode Num: 117 Episode T: 230 Reward: 81.694\n",
      "Total T: 77957 Episode Num: 118 Episode T: 185 Reward: 85.940\n",
      "Total T: 78111 Episode Num: 119 Episode T: 154 Reward: 88.130\n",
      "Total T: 78345 Episode Num: 120 Episode T: 234 Reward: 81.508\n",
      "Total T: 78590 Episode Num: 121 Episode T: 245 Reward: 81.134\n",
      "Total T: 78810 Episode Num: 122 Episode T: 220 Reward: 82.362\n",
      "Total T: 79050 Episode Num: 123 Episode T: 240 Reward: 81.551\n",
      "Total T: 79292 Episode Num: 124 Episode T: 242 Reward: 82.140\n",
      "Total T: 79492 Episode Num: 125 Episode T: 200 Reward: 84.423\n",
      "Total T: 79717 Episode Num: 126 Episode T: 225 Reward: 82.671\n",
      "Total T: 79875 Episode Num: 127 Episode T: 158 Reward: 87.929\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: 92.690\n",
      "---------------------------------------\n",
      "Total T: 80080 Episode Num: 128 Episode T: 205 Reward: 84.453\n",
      "Total T: 80262 Episode Num: 129 Episode T: 182 Reward: 85.217\n",
      "Total T: 80506 Episode Num: 130 Episode T: 244 Reward: 81.424\n",
      "Total T: 80664 Episode Num: 131 Episode T: 158 Reward: 86.914\n",
      "Total T: 80825 Episode Num: 132 Episode T: 161 Reward: 87.669\n",
      "Total T: 81073 Episode Num: 133 Episode T: 248 Reward: 80.961\n",
      "Total T: 81356 Episode Num: 134 Episode T: 283 Reward: 77.806\n",
      "Total T: 81534 Episode Num: 135 Episode T: 178 Reward: 87.020\n",
      "Total T: 81774 Episode Num: 136 Episode T: 240 Reward: 81.751\n",
      "Total T: 81930 Episode Num: 137 Episode T: 156 Reward: 88.586\n",
      "Total T: 82164 Episode Num: 138 Episode T: 234 Reward: 82.070\n",
      "Total T: 82420 Episode Num: 139 Episode T: 256 Reward: 79.214\n",
      "Total T: 82619 Episode Num: 140 Episode T: 199 Reward: 85.011\n",
      "Total T: 82776 Episode Num: 141 Episode T: 157 Reward: 87.335\n",
      "Total T: 82923 Episode Num: 142 Episode T: 147 Reward: 88.420\n",
      "Total T: 83075 Episode Num: 143 Episode T: 152 Reward: 87.794\n",
      "Total T: 83347 Episode Num: 144 Episode T: 272 Reward: 78.095\n",
      "Total T: 83484 Episode Num: 145 Episode T: 137 Reward: 89.435\n",
      "Total T: 83722 Episode Num: 146 Episode T: 238 Reward: 80.707\n",
      "Total T: 83840 Episode Num: 147 Episode T: 118 Reward: 90.878\n",
      "Total T: 83986 Episode Num: 148 Episode T: 146 Reward: 88.456\n",
      "Total T: 84209 Episode Num: 149 Episode T: 223 Reward: 83.077\n",
      "Total T: 84368 Episode Num: 150 Episode T: 159 Reward: 88.134\n",
      "Total T: 84562 Episode Num: 151 Episode T: 194 Reward: 85.082\n",
      "Total T: 84735 Episode Num: 152 Episode T: 173 Reward: 86.326\n",
      "Total T: 84921 Episode Num: 153 Episode T: 186 Reward: 85.705\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: 92.785\n",
      "---------------------------------------\n",
      "Total T: 85153 Episode Num: 154 Episode T: 232 Reward: 81.640\n",
      "Total T: 85305 Episode Num: 155 Episode T: 152 Reward: 88.442\n",
      "Total T: 85459 Episode Num: 156 Episode T: 154 Reward: 88.359\n",
      "Total T: 85695 Episode Num: 157 Episode T: 236 Reward: 83.160\n",
      "Total T: 86000 Episode Num: 158 Episode T: 305 Reward: 74.948\n",
      "Total T: 86153 Episode Num: 159 Episode T: 153 Reward: 87.268\n",
      "Total T: 86379 Episode Num: 160 Episode T: 226 Reward: 82.627\n",
      "Total T: 86588 Episode Num: 161 Episode T: 209 Reward: 83.796\n",
      "Total T: 86803 Episode Num: 162 Episode T: 215 Reward: 84.663\n",
      "Total T: 87096 Episode Num: 163 Episode T: 293 Reward: 76.827\n",
      "Total T: 87253 Episode Num: 164 Episode T: 157 Reward: 88.860\n",
      "Total T: 87409 Episode Num: 165 Episode T: 156 Reward: 88.164\n",
      "Total T: 87553 Episode Num: 166 Episode T: 144 Reward: 89.388\n",
      "Total T: 87813 Episode Num: 167 Episode T: 260 Reward: 80.739\n",
      "Total T: 88073 Episode Num: 168 Episode T: 260 Reward: 80.534\n",
      "Total T: 88313 Episode Num: 169 Episode T: 240 Reward: 82.046\n",
      "Total T: 88675 Episode Num: 170 Episode T: 362 Reward: 72.014\n",
      "Total T: 88906 Episode Num: 171 Episode T: 231 Reward: 81.839\n",
      "Total T: 89139 Episode Num: 172 Episode T: 233 Reward: 81.475\n",
      "Total T: 89365 Episode Num: 173 Episode T: 226 Reward: 82.422\n",
      "Total T: 89683 Episode Num: 174 Episode T: 318 Reward: 75.036\n",
      "Total T: 89847 Episode Num: 175 Episode T: 164 Reward: 86.604\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: 92.890\n",
      "---------------------------------------\n",
      "Total T: 90081 Episode Num: 176 Episode T: 234 Reward: 82.047\n",
      "Total T: 90225 Episode Num: 177 Episode T: 144 Reward: 88.033\n",
      "Total T: 90461 Episode Num: 178 Episode T: 236 Reward: 81.962\n",
      "Total T: 90685 Episode Num: 179 Episode T: 224 Reward: 82.693\n",
      "Total T: 90848 Episode Num: 180 Episode T: 163 Reward: 87.519\n",
      "Total T: 91005 Episode Num: 181 Episode T: 157 Reward: 88.176\n",
      "Total T: 91234 Episode Num: 182 Episode T: 229 Reward: 81.995\n",
      "Total T: 91409 Episode Num: 183 Episode T: 175 Reward: 86.392\n",
      "Total T: 91664 Episode Num: 184 Episode T: 255 Reward: 80.529\n",
      "Total T: 91818 Episode Num: 185 Episode T: 154 Reward: 88.376\n",
      "Total T: 91997 Episode Num: 186 Episode T: 179 Reward: 87.176\n",
      "Total T: 92133 Episode Num: 187 Episode T: 136 Reward: 89.185\n",
      "Total T: 92290 Episode Num: 188 Episode T: 157 Reward: 88.426\n",
      "Total T: 92574 Episode Num: 189 Episode T: 284 Reward: 78.972\n",
      "Total T: 92806 Episode Num: 190 Episode T: 232 Reward: 81.927\n",
      "Total T: 92946 Episode Num: 191 Episode T: 140 Reward: 89.151\n",
      "Total T: 93096 Episode Num: 192 Episode T: 150 Reward: 88.343\n",
      "Total T: 93237 Episode Num: 193 Episode T: 141 Reward: 88.934\n",
      "Total T: 93547 Episode Num: 194 Episode T: 310 Reward: 76.431\n",
      "Total T: 93695 Episode Num: 195 Episode T: 148 Reward: 88.276\n",
      "Total T: 93849 Episode Num: 196 Episode T: 154 Reward: 87.433\n",
      "Total T: 94027 Episode Num: 197 Episode T: 178 Reward: 86.168\n",
      "Total T: 94379 Episode Num: 198 Episode T: 352 Reward: 72.199\n",
      "Total T: 94536 Episode Num: 199 Episode T: 157 Reward: 87.345\n",
      "Total T: 94682 Episode Num: 200 Episode T: 146 Reward: 88.102\n",
      "Total T: 94905 Episode Num: 201 Episode T: 223 Reward: 83.566\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: 93.065\n",
      "---------------------------------------\n",
      "Total T: 95213 Episode Num: 202 Episode T: 308 Reward: 76.179\n",
      "Total T: 95369 Episode Num: 203 Episode T: 156 Reward: 87.888\n",
      "Total T: 95611 Episode Num: 204 Episode T: 242 Reward: 81.344\n",
      "Total T: 95766 Episode Num: 205 Episode T: 155 Reward: 88.676\n",
      "Total T: 95906 Episode Num: 206 Episode T: 140 Reward: 89.255\n",
      "Total T: 96154 Episode Num: 207 Episode T: 248 Reward: 80.873\n",
      "Total T: 96357 Episode Num: 208 Episode T: 203 Reward: 84.040\n",
      "Total T: 96592 Episode Num: 209 Episode T: 235 Reward: 82.783\n",
      "Total T: 96820 Episode Num: 210 Episode T: 228 Reward: 82.377\n",
      "Total T: 97051 Episode Num: 211 Episode T: 231 Reward: 82.463\n",
      "Total T: 97215 Episode Num: 212 Episode T: 164 Reward: 87.186\n",
      "Total T: 97415 Episode Num: 213 Episode T: 200 Reward: 84.499\n",
      "Total T: 97572 Episode Num: 214 Episode T: 157 Reward: 88.163\n",
      "Total T: 97725 Episode Num: 215 Episode T: 153 Reward: 88.073\n",
      "Total T: 97958 Episode Num: 216 Episode T: 233 Reward: 81.777\n",
      "Total T: 98183 Episode Num: 217 Episode T: 225 Reward: 83.376\n",
      "Total T: 98399 Episode Num: 218 Episode T: 216 Reward: 82.474\n",
      "Total T: 98560 Episode Num: 219 Episode T: 161 Reward: 87.476\n",
      "Total T: 98820 Episode Num: 220 Episode T: 260 Reward: 79.889\n",
      "Total T: 98981 Episode Num: 221 Episode T: 161 Reward: 87.638\n",
      "Total T: 99130 Episode Num: 222 Episode T: 149 Reward: 88.455\n",
      "Total T: 99385 Episode Num: 223 Episode T: 255 Reward: 79.925\n",
      "Total T: 99621 Episode Num: 224 Episode T: 236 Reward: 82.076\n",
      "Total T: 99771 Episode Num: 225 Episode T: 150 Reward: 88.247\n",
      "Total T: 99913 Episode Num: 226 Episode T: 142 Reward: 88.749\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: 93.126\n",
      "---------------------------------------\n",
      "Total T: 100151 Episode Num: 227 Episode T: 238 Reward: 82.773\n",
      "Total T: 100401 Episode Num: 228 Episode T: 250 Reward: 80.675\n",
      "Total T: 100555 Episode Num: 229 Episode T: 154 Reward: 88.112\n",
      "Total T: 100707 Episode Num: 230 Episode T: 152 Reward: 88.863\n",
      "Total T: 100873 Episode Num: 231 Episode T: 166 Reward: 87.381\n",
      "Total T: 101027 Episode Num: 232 Episode T: 154 Reward: 87.665\n",
      "Total T: 101279 Episode Num: 233 Episode T: 252 Reward: 80.391\n",
      "Total T: 101453 Episode Num: 234 Episode T: 174 Reward: 86.778\n",
      "Total T: 101664 Episode Num: 235 Episode T: 211 Reward: 83.306\n",
      "Total T: 101922 Episode Num: 236 Episode T: 258 Reward: 79.502\n",
      "Total T: 102149 Episode Num: 237 Episode T: 227 Reward: 82.616\n",
      "Total T: 102300 Episode Num: 238 Episode T: 151 Reward: 88.484\n",
      "Total T: 102476 Episode Num: 239 Episode T: 176 Reward: 86.267\n",
      "Total T: 102733 Episode Num: 240 Episode T: 257 Reward: 80.656\n",
      "Total T: 102997 Episode Num: 241 Episode T: 264 Reward: 79.195\n",
      "Total T: 103226 Episode Num: 242 Episode T: 229 Reward: 81.840\n",
      "Total T: 103467 Episode Num: 243 Episode T: 241 Reward: 81.958\n",
      "Total T: 103706 Episode Num: 244 Episode T: 239 Reward: 81.209\n",
      "Total T: 103858 Episode Num: 245 Episode T: 152 Reward: 88.471\n",
      "Total T: 104097 Episode Num: 246 Episode T: 239 Reward: 82.100\n",
      "Total T: 104419 Episode Num: 247 Episode T: 322 Reward: 75.148\n",
      "Total T: 104606 Episode Num: 248 Episode T: 187 Reward: 85.519\n",
      "Total T: 104915 Episode Num: 249 Episode T: 309 Reward: 76.214\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: 93.162\n",
      "---------------------------------------\n",
      "Total T: 105178 Episode Num: 250 Episode T: 263 Reward: 80.235\n",
      "Total T: 105472 Episode Num: 251 Episode T: 294 Reward: 77.258\n",
      "Total T: 105716 Episode Num: 252 Episode T: 244 Reward: 80.251\n",
      "Total T: 105977 Episode Num: 253 Episode T: 261 Reward: 80.158\n",
      "Total T: 106200 Episode Num: 254 Episode T: 223 Reward: 83.348\n",
      "Total T: 106352 Episode Num: 255 Episode T: 152 Reward: 88.521\n",
      "Total T: 106597 Episode Num: 256 Episode T: 245 Reward: 80.497\n",
      "Total T: 106842 Episode Num: 257 Episode T: 245 Reward: 80.774\n",
      "Total T: 107138 Episode Num: 258 Episode T: 296 Reward: 77.292\n",
      "Total T: 107367 Episode Num: 259 Episode T: 229 Reward: 82.547\n",
      "Total T: 107601 Episode Num: 260 Episode T: 234 Reward: 81.666\n",
      "Total T: 107867 Episode Num: 261 Episode T: 266 Reward: 78.927\n",
      "Total T: 108100 Episode Num: 262 Episode T: 233 Reward: 81.864\n",
      "Total T: 108370 Episode Num: 263 Episode T: 270 Reward: 79.058\n",
      "Total T: 108585 Episode Num: 264 Episode T: 215 Reward: 83.465\n",
      "Total T: 108809 Episode Num: 265 Episode T: 224 Reward: 82.984\n",
      "Total T: 109033 Episode Num: 266 Episode T: 224 Reward: 81.617\n",
      "Total T: 109257 Episode Num: 267 Episode T: 224 Reward: 83.146\n",
      "Total T: 109486 Episode Num: 268 Episode T: 229 Reward: 83.519\n",
      "Total T: 109778 Episode Num: 269 Episode T: 292 Reward: 78.845\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: 93.167\n",
      "---------------------------------------\n",
      "Total T: 110007 Episode Num: 270 Episode T: 229 Reward: 82.946\n",
      "Total T: 110255 Episode Num: 271 Episode T: 248 Reward: 80.800\n",
      "Total T: 110401 Episode Num: 272 Episode T: 146 Reward: 88.242\n",
      "Total T: 110639 Episode Num: 273 Episode T: 238 Reward: 81.529\n",
      "Total T: 110794 Episode Num: 274 Episode T: 155 Reward: 88.551\n",
      "Total T: 111016 Episode Num: 275 Episode T: 222 Reward: 83.496\n",
      "Total T: 111172 Episode Num: 276 Episode T: 156 Reward: 88.669\n",
      "Total T: 111316 Episode Num: 277 Episode T: 144 Reward: 88.946\n",
      "Total T: 111502 Episode Num: 278 Episode T: 186 Reward: 85.494\n",
      "Total T: 111751 Episode Num: 279 Episode T: 249 Reward: 81.664\n",
      "Total T: 111911 Episode Num: 280 Episode T: 160 Reward: 87.329\n",
      "Total T: 112144 Episode Num: 281 Episode T: 233 Reward: 83.106\n",
      "Total T: 112379 Episode Num: 282 Episode T: 235 Reward: 82.098\n",
      "Total T: 112550 Episode Num: 283 Episode T: 171 Reward: 87.683\n",
      "Total T: 112706 Episode Num: 284 Episode T: 156 Reward: 88.435\n",
      "Total T: 112866 Episode Num: 285 Episode T: 160 Reward: 87.891\n",
      "Total T: 113090 Episode Num: 286 Episode T: 224 Reward: 82.091\n",
      "Total T: 113252 Episode Num: 287 Episode T: 162 Reward: 87.416\n",
      "Total T: 113471 Episode Num: 288 Episode T: 219 Reward: 83.275\n",
      "Total T: 113685 Episode Num: 289 Episode T: 214 Reward: 82.906\n",
      "Total T: 113957 Episode Num: 290 Episode T: 272 Reward: 80.309\n",
      "Total T: 114249 Episode Num: 291 Episode T: 292 Reward: 77.815\n",
      "Total T: 114512 Episode Num: 292 Episode T: 263 Reward: 79.516\n",
      "Total T: 114839 Episode Num: 293 Episode T: 327 Reward: 74.718\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: 93.184\n",
      "---------------------------------------\n",
      "Total T: 115078 Episode Num: 294 Episode T: 239 Reward: 81.791\n",
      "Total T: 115227 Episode Num: 295 Episode T: 149 Reward: 88.805\n",
      "Total T: 115389 Episode Num: 296 Episode T: 162 Reward: 87.126\n",
      "Total T: 115552 Episode Num: 297 Episode T: 163 Reward: 87.148\n",
      "Total T: 115703 Episode Num: 298 Episode T: 151 Reward: 88.585\n",
      "Total T: 116012 Episode Num: 299 Episode T: 309 Reward: 76.376\n",
      "Total T: 116240 Episode Num: 300 Episode T: 228 Reward: 81.035\n",
      "Total T: 116484 Episode Num: 301 Episode T: 244 Reward: 81.831\n",
      "Total T: 116713 Episode Num: 302 Episode T: 229 Reward: 82.762\n",
      "Total T: 116899 Episode Num: 303 Episode T: 186 Reward: 85.878\n",
      "Total T: 117052 Episode Num: 304 Episode T: 153 Reward: 87.747\n",
      "Total T: 117385 Episode Num: 305 Episode T: 333 Reward: 74.515\n",
      "Total T: 117532 Episode Num: 306 Episode T: 147 Reward: 88.068\n",
      "Total T: 117782 Episode Num: 307 Episode T: 250 Reward: 80.597\n",
      "Total T: 117941 Episode Num: 308 Episode T: 159 Reward: 88.216\n",
      "Total T: 118099 Episode Num: 309 Episode T: 158 Reward: 87.766\n",
      "Total T: 118320 Episode Num: 310 Episode T: 221 Reward: 82.969\n",
      "Total T: 118567 Episode Num: 311 Episode T: 247 Reward: 81.372\n",
      "Total T: 118807 Episode Num: 312 Episode T: 240 Reward: 81.298\n",
      "Total T: 118957 Episode Num: 313 Episode T: 150 Reward: 89.282\n",
      "Total T: 119109 Episode Num: 314 Episode T: 152 Reward: 88.356\n",
      "Total T: 119266 Episode Num: 315 Episode T: 157 Reward: 88.119\n",
      "Total T: 119488 Episode Num: 316 Episode T: 222 Reward: 82.776\n",
      "Total T: 119698 Episode Num: 317 Episode T: 210 Reward: 83.771\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: 93.200\n",
      "---------------------------------------\n",
      "Total T: 120010 Episode Num: 318 Episode T: 312 Reward: 76.880\n",
      "Total T: 120322 Episode Num: 319 Episode T: 312 Reward: 75.078\n",
      "Total T: 120539 Episode Num: 320 Episode T: 217 Reward: 82.819\n",
      "Total T: 120767 Episode Num: 321 Episode T: 228 Reward: 82.266\n",
      "Total T: 120984 Episode Num: 322 Episode T: 217 Reward: 82.591\n",
      "Total T: 121149 Episode Num: 323 Episode T: 165 Reward: 87.726\n",
      "Total T: 121380 Episode Num: 324 Episode T: 231 Reward: 83.126\n",
      "Total T: 121471 Episode Num: 325 Episode T: 91 Reward: 92.695\n",
      "Total T: 121754 Episode Num: 326 Episode T: 283 Reward: 78.594\n",
      "Total T: 122220 Episode Num: 327 Episode T: 466 Reward: 65.380\n",
      "Total T: 122372 Episode Num: 328 Episode T: 152 Reward: 88.164\n",
      "Total T: 122667 Episode Num: 329 Episode T: 295 Reward: 77.025\n",
      "Total T: 122890 Episode Num: 330 Episode T: 223 Reward: 82.994\n",
      "Total T: 123048 Episode Num: 331 Episode T: 158 Reward: 87.709\n",
      "Total T: 123558 Episode Num: 332 Episode T: 510 Reward: 62.473\n",
      "Total T: 123770 Episode Num: 333 Episode T: 212 Reward: 83.750\n",
      "Total T: 124079 Episode Num: 334 Episode T: 309 Reward: 75.276\n",
      "Total T: 124297 Episode Num: 335 Episode T: 218 Reward: 82.633\n",
      "Total T: 124531 Episode Num: 336 Episode T: 234 Reward: 81.868\n",
      "Total T: 124777 Episode Num: 337 Episode T: 246 Reward: 81.596\n",
      "Total T: 124997 Episode Num: 338 Episode T: 220 Reward: 82.749\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: 93.323\n",
      "---------------------------------------\n",
      "Total T: 125312 Episode Num: 339 Episode T: 315 Reward: 76.263\n",
      "Total T: 125489 Episode Num: 340 Episode T: 177 Reward: 86.685\n",
      "Total T: 125739 Episode Num: 341 Episode T: 250 Reward: 81.150\n",
      "Total T: 125960 Episode Num: 342 Episode T: 221 Reward: 83.521\n",
      "Total T: 126410 Episode Num: 343 Episode T: 450 Reward: 65.653\n",
      "Total T: 126635 Episode Num: 344 Episode T: 225 Reward: 82.143\n",
      "Total T: 126932 Episode Num: 345 Episode T: 297 Reward: 77.115\n",
      "Total T: 127243 Episode Num: 346 Episode T: 311 Reward: 75.952\n",
      "Total T: 127584 Episode Num: 347 Episode T: 341 Reward: 74.460\n",
      "Total T: 127826 Episode Num: 348 Episode T: 242 Reward: 81.548\n",
      "Total T: 128227 Episode Num: 349 Episode T: 401 Reward: 69.750\n",
      "Total T: 128371 Episode Num: 350 Episode T: 144 Reward: 88.924\n",
      "Total T: 128530 Episode Num: 351 Episode T: 159 Reward: 88.269\n",
      "Total T: 128803 Episode Num: 352 Episode T: 273 Reward: 78.776\n",
      "Total T: 129015 Episode Num: 353 Episode T: 212 Reward: 83.135\n",
      "Total T: 129387 Episode Num: 354 Episode T: 372 Reward: 70.135\n",
      "Total T: 129678 Episode Num: 355 Episode T: 291 Reward: 78.349\n",
      "Total T: 129893 Episode Num: 356 Episode T: 215 Reward: 82.712\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: 93.393\n",
      "---------------------------------------\n",
      "Total T: 130121 Episode Num: 357 Episode T: 228 Reward: 82.222\n",
      "Total T: 130393 Episode Num: 358 Episode T: 272 Reward: 78.912\n",
      "Total T: 130598 Episode Num: 359 Episode T: 205 Reward: 84.092\n",
      "Total T: 130914 Episode Num: 360 Episode T: 316 Reward: 74.973\n",
      "Total T: 131162 Episode Num: 361 Episode T: 248 Reward: 80.407\n",
      "Total T: 131470 Episode Num: 362 Episode T: 308 Reward: 76.642\n",
      "Total T: 131767 Episode Num: 363 Episode T: 297 Reward: 77.233\n",
      "Total T: 132127 Episode Num: 364 Episode T: 360 Reward: 73.063\n",
      "Total T: 132546 Episode Num: 365 Episode T: 419 Reward: 67.594\n",
      "Total T: 132852 Episode Num: 366 Episode T: 306 Reward: 77.400\n",
      "Total T: 133070 Episode Num: 367 Episode T: 218 Reward: 83.419\n",
      "Total T: 133280 Episode Num: 368 Episode T: 210 Reward: 83.285\n",
      "Total T: 133499 Episode Num: 369 Episode T: 219 Reward: 83.652\n",
      "Total T: 134072 Episode Num: 370 Episode T: 573 Reward: 56.031\n",
      "Total T: 134300 Episode Num: 371 Episode T: 228 Reward: 82.195\n",
      "Total T: 134536 Episode Num: 372 Episode T: 236 Reward: 82.348\n",
      "Total T: 134754 Episode Num: 373 Episode T: 218 Reward: 82.506\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: 93.356\n",
      "---------------------------------------\n",
      "Total T: 135078 Episode Num: 374 Episode T: 324 Reward: 73.892\n",
      "Total T: 135309 Episode Num: 375 Episode T: 231 Reward: 81.538\n",
      "Total T: 135546 Episode Num: 376 Episode T: 237 Reward: 81.960\n",
      "Total T: 135873 Episode Num: 377 Episode T: 327 Reward: 74.252\n",
      "Total T: 136126 Episode Num: 378 Episode T: 253 Reward: 80.411\n",
      "Total T: 136515 Episode Num: 379 Episode T: 389 Reward: 69.026\n",
      "Total T: 136927 Episode Num: 380 Episode T: 412 Reward: 67.526\n",
      "Total T: 137207 Episode Num: 381 Episode T: 280 Reward: 77.470\n",
      "Total T: 137568 Episode Num: 382 Episode T: 361 Reward: 71.817\n",
      "Total T: 137720 Episode Num: 383 Episode T: 152 Reward: 87.968\n",
      "Total T: 138025 Episode Num: 384 Episode T: 305 Reward: 77.352\n",
      "Total T: 138195 Episode Num: 385 Episode T: 170 Reward: 87.008\n",
      "Total T: 138345 Episode Num: 386 Episode T: 150 Reward: 87.825\n",
      "Total T: 138642 Episode Num: 387 Episode T: 297 Reward: 77.088\n",
      "Total T: 139048 Episode Num: 388 Episode T: 406 Reward: 67.431\n",
      "Total T: 139293 Episode Num: 389 Episode T: 245 Reward: 80.912\n",
      "Total T: 139610 Episode Num: 390 Episode T: 317 Reward: 76.042\n",
      "Total T: 139768 Episode Num: 391 Episode T: 158 Reward: 87.488\n",
      "Total T: 139929 Episode Num: 392 Episode T: 161 Reward: 87.637\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: 93.413\n",
      "---------------------------------------\n",
      "Total T: 140198 Episode Num: 393 Episode T: 269 Reward: 78.889\n",
      "Total T: 140354 Episode Num: 394 Episode T: 156 Reward: 87.739\n",
      "Total T: 140657 Episode Num: 395 Episode T: 303 Reward: 77.045\n",
      "Total T: 140991 Episode Num: 396 Episode T: 334 Reward: 74.664\n",
      "Total T: 141200 Episode Num: 397 Episode T: 209 Reward: 83.358\n",
      "Total T: 141445 Episode Num: 398 Episode T: 245 Reward: 82.003\n",
      "Total T: 141669 Episode Num: 399 Episode T: 224 Reward: 81.788\n",
      "Total T: 142317 Episode Num: 400 Episode T: 648 Reward: 49.679\n",
      "Total T: 142634 Episode Num: 401 Episode T: 317 Reward: 75.954\n",
      "Total T: 142875 Episode Num: 402 Episode T: 241 Reward: 80.919\n",
      "Total T: 143104 Episode Num: 403 Episode T: 229 Reward: 83.016\n",
      "Total T: 143380 Episode Num: 404 Episode T: 276 Reward: 79.680\n",
      "Total T: 143846 Episode Num: 405 Episode T: 466 Reward: 63.858\n",
      "Total T: 144154 Episode Num: 406 Episode T: 308 Reward: 76.023\n",
      "Total T: 144476 Episode Num: 407 Episode T: 322 Reward: 75.707\n",
      "Total T: 144710 Episode Num: 408 Episode T: 234 Reward: 81.131\n",
      "Total T: 144953 Episode Num: 409 Episode T: 243 Reward: 80.467\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: 93.486\n",
      "---------------------------------------\n",
      "Total T: 145311 Episode Num: 410 Episode T: 358 Reward: 72.371\n",
      "Total T: 145572 Episode Num: 411 Episode T: 261 Reward: 79.118\n",
      "Total T: 145804 Episode Num: 412 Episode T: 232 Reward: 82.915\n",
      "Total T: 146042 Episode Num: 413 Episode T: 238 Reward: 81.112\n",
      "Total T: 146272 Episode Num: 414 Episode T: 230 Reward: 82.901\n",
      "Total T: 146490 Episode Num: 415 Episode T: 218 Reward: 84.237\n",
      "Total T: 146639 Episode Num: 416 Episode T: 149 Reward: 88.654\n",
      "Total T: 147190 Episode Num: 417 Episode T: 551 Reward: 58.810\n",
      "Total T: 147342 Episode Num: 418 Episode T: 152 Reward: 88.967\n",
      "Total T: 147642 Episode Num: 419 Episode T: 300 Reward: 77.907\n",
      "Total T: 147787 Episode Num: 420 Episode T: 145 Reward: 88.904\n",
      "Total T: 148034 Episode Num: 421 Episode T: 247 Reward: 81.909\n",
      "Total T: 148247 Episode Num: 422 Episode T: 213 Reward: 83.546\n",
      "Total T: 148534 Episode Num: 423 Episode T: 287 Reward: 77.567\n",
      "Total T: 148759 Episode Num: 424 Episode T: 225 Reward: 82.356\n",
      "Total T: 149037 Episode Num: 425 Episode T: 278 Reward: 79.124\n",
      "Total T: 149193 Episode Num: 426 Episode T: 156 Reward: 88.742\n",
      "Total T: 149421 Episode Num: 427 Episode T: 228 Reward: 81.974\n",
      "Total T: 149586 Episode Num: 428 Episode T: 165 Reward: 86.995\n",
      "Total T: 149974 Episode Num: 429 Episode T: 388 Reward: 69.347\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: 93.415\n",
      "---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# MAIN FILE\n",
    "\n",
    "\n",
    "# Runs policy for X episodes and returns average reward\n",
    "# A fixed seed is used for the eval environment\n",
    "def eval_policy(policy, env_name, seed, eval_episodes=10):\n",
    "\teval_env = gym.make(env_name)\n",
    "\teval_env.seed(seed + 100)\n",
    "\n",
    "\tavg_reward = 0.\n",
    "\tfor _ in range(eval_episodes):\n",
    "\t\tstate, done = eval_env.reset(), False\n",
    "\t\twhile not done:\n",
    "\t\t\taction = policy.select_action(np.array(state))\n",
    "\t\t\tstate, reward, done, _ = eval_env.step(action)\n",
    "\t\t\tavg_reward += reward\n",
    "\n",
    "\tavg_reward /= eval_episodes\n",
    "\n",
    "\tprint(\"---------------------------------------\")\n",
    "\tprint(f\"Evaluation over {eval_episodes} episodes: {avg_reward:.3f}\")\n",
    "\tprint(\"---------------------------------------\")\n",
    "\treturn avg_reward\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\t\n",
    "\t'''\n",
    "\tparser = argparse.ArgumentParser()\n",
    "\tparser.add_argument(\"--policy\", default=\"TD3\")                  # Policy name (TD3, DDPG or OurDDPG)\n",
    "\tparser.add_argument(\"--env\", default=\"HalfCheetah-v2\")          # OpenAI gym environment name\n",
    "\tparser.add_argument(\"--seed\", default=0, type=int)              # Sets Gym, PyTorch and Numpy seeds\n",
    "\tparser.add_argument(\"--start_timesteps\", default=25e3, type=int)# Time steps initial random policy is used\n",
    "\tparser.add_argument(\"--eval_freq\", default=5e3, type=int)       # How often (time steps) we evaluate\n",
    "\tparser.add_argument(\"--max_timesteps\", default=1e6, type=int)   # Max time steps to run environment\n",
    "\tparser.add_argument(\"--expl_noise\", default=0.1, type=float)    # Std of Gaussian exploration noise\n",
    "\tparser.add_argument(\"--batch_size\", default=256, type=int)      # Batch size for both actor and critic\n",
    "\tparser.add_argument(\"--discount\", default=0.99, type=float)     # Discount factor\n",
    "\tparser.add_argument(\"--tau\", default=0.005, type=float)         # Target network update rate\n",
    "\tparser.add_argument(\"--policy_noise\", default=0.2)              # Noise added to target policy during critic update\n",
    "\tparser.add_argument(\"--noise_clip\", default=0.5)                # Range to clip target policy noise\n",
    "\tparser.add_argument(\"--policy_freq\", default=2, type=int)       # Frequency of delayed policy updates\n",
    "\tparser.add_argument(\"--save_model\", action=\"store_true\")        # Save model and optimizer parameters\n",
    "\tparser.add_argument(\"--load_model\", default=\"\")                 # Model load file name, \"\" doesn't load, \"default\" uses file_name\n",
    "\targs = parser.parse_args()\n",
    "\t'''\n",
    "\targs = Args()\n",
    "\n",
    "\tfile_name = f\"{args.policy}_{args.env}_{args.seed}\"\n",
    "\tprint(\"---------------------------------------\")\n",
    "\tprint(f\"Policy: {args.policy}, Env: {args.env}, Seed: {args.seed}\")\n",
    "\tprint(\"---------------------------------------\")\n",
    "\n",
    "\tif not os.path.exists(\"./results\"):\n",
    "\t\tos.makedirs(\"./results\")\n",
    "\n",
    "\tif args.save_model and not os.path.exists(\"./models\"):\n",
    "\t\tos.makedirs(\"./models\")\n",
    "\n",
    "\tenv = gym.make(args.env)\n",
    "\n",
    "\t# Set seeds\n",
    "\tenv.seed(args.seed)\n",
    "\tenv.action_space.seed(args.seed)\n",
    "\ttorch.manual_seed(args.seed)\n",
    "\tnp.random.seed(args.seed)\n",
    "\t\n",
    "\tstate_dim = env.observation_space.shape[0]\n",
    "\taction_dim = env.action_space.shape[0] \n",
    "\tmax_action = float(env.action_space.high[0])\n",
    "\n",
    "\tkwargs = {\n",
    "\t\t\"state_dim\": state_dim,\n",
    "\t\t\"action_dim\": action_dim,\n",
    "\t\t\"max_action\": max_action,\n",
    "\t\t\"discount\": args.discount,\n",
    "\t\t\"tau\": args.tau,\n",
    "\t}\n",
    "\n",
    "\t# Initialize policy\n",
    "\tif args.policy == \"TD3\":\n",
    "\t\t# Target policy smoothing is scaled wrt the action scale\n",
    "\t\tkwargs[\"policy_noise\"] = args.policy_noise * max_action\n",
    "\t\tkwargs[\"noise_clip\"] = args.noise_clip * max_action\n",
    "\t\tkwargs[\"policy_freq\"] = args.policy_freq\n",
    "\t\t#policy = TD3.TD3(**kwargs)\n",
    "\t\tpolicy = TD3(**kwargs)\n",
    "\telif args.policy == \"OurDDPG\":\n",
    "\t\tpolicy = OurDDPG.DDPG(**kwargs)\n",
    "\telif args.policy == \"DDPG\":\n",
    "\t\tpolicy = DDPG.DDPG(**kwargs)\n",
    "\n",
    "\tif args.load_model != \"\":\n",
    "\t\tpolicy_file = file_name if args.load_model == \"default\" else args.load_model\n",
    "\t\tpolicy.load(f\"./models/{policy_file}\")\n",
    "\n",
    "\treplay_buffer = utils.ReplayBuffer(state_dim, action_dim)\n",
    "\t\n",
    "\t# Evaluate untrained policy\n",
    "\tevaluations = [eval_policy(policy, args.env, args.seed)]\n",
    "\n",
    "\tstate, done = env.reset(), False\n",
    "\tepisode_reward = 0\n",
    "\tepisode_timesteps = 0\n",
    "\tepisode_num = 0\n",
    "\n",
    "\n",
    "\tif args.load_model == \"\":\n",
    "\t\tfor t in range(int(args.max_timesteps)):\n",
    "\t\t\t\n",
    "\t\t\tepisode_timesteps += 1\n",
    "\n",
    "\t\t\t# Select action randomly or according to policy\n",
    "\t\t\tif t < args.start_timesteps:\n",
    "\t\t\t\taction = env.action_space.sample()\n",
    "\t\t\telse:\n",
    "\t\t\t\taction = (\n",
    "\t\t\t\t\tpolicy.select_action(np.array(state))\n",
    "\t\t\t\t\t+ np.random.normal(0, max_action * args.expl_noise, size=action_dim)\n",
    "\t\t\t\t).clip(-max_action, max_action)\n",
    "\n",
    "\t\t\t# Perform action\n",
    "\t\t\tnext_state, reward, done, _ = env.step(action) \n",
    "\t\t\tdone_bool = float(done) if episode_timesteps < env._max_episode_steps else 0\n",
    "\n",
    "\t\t\t# Store data in replay buffer\n",
    "\t\t\treplay_buffer.add(state, action, next_state, reward, done_bool)\n",
    "\n",
    "\t\t\tstate = next_state\n",
    "\t\t\tepisode_reward += reward\n",
    "\n",
    "\t\t\t# Train agent after collecting sufficient data\n",
    "\t\t\tif t >= args.start_timesteps:\n",
    "\t\t\t\tpolicy.train(replay_buffer, args.batch_size)\n",
    "\n",
    "\t\t\tif done: \n",
    "\t\t\t\t# +1 to account for 0 indexing. +0 on ep_timesteps since it will increment +1 even if done=True\n",
    "\t\t\t\tprint(f\"Total T: {t+1} Episode Num: {episode_num+1} Episode T: {episode_timesteps} Reward: {episode_reward:.3f}\")\n",
    "\t\t\t\t# Reset environment\n",
    "\t\t\t\tstate, done = env.reset(), False\n",
    "\t\t\t\tepisode_reward = 0\n",
    "\t\t\t\tepisode_timesteps = 0\n",
    "\t\t\t\tepisode_num += 1 \n",
    "\n",
    "\t\t\t# Evaluate episode\n",
    "\t\t\tif (t + 1) % args.eval_freq == 0:\n",
    "\t\t\t\tevaluations.append(eval_policy(policy, args.env, args.seed))\n",
    "\t\t\t\tnp.save(f\"./results/{file_name}\", evaluations)\n",
    "\t\t\t\tif args.save_model: policy.save(f\"./models/{file_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\simon\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    }
   ],
   "source": [
    "# Visualization\n",
    "\n",
    "env.close()\n",
    "# Assuming 'device' and 'act' are defined elsewhere in your script\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize the continuous version of the Mountain Car environment\n",
    "env = gym.make(\"MountainCarContinuous-v0\", render_mode=\"human\")\n",
    "\n",
    "# Reset the environment and prepare the initial state\n",
    "state, done = env.reset(), False\n",
    "state = torch.tensor(state, device=device).float().unsqueeze(0)\n",
    "\n",
    "# Initialize variables to track episodes and scores\n",
    "ep_count = 0\n",
    "current_ep_reward = 0\n",
    "scores = []\n",
    "\n",
    "# Run simulation for 50 episodes\n",
    "while ep_count < 50:\n",
    "    env.render()\n",
    "    action = policy.select_action(np.array(state))  # Ensure this function now returns a continuous action\n",
    "    state, reward, done, _ = env.step([action.item()])  # Execute action, note the list around action.item()\n",
    "\n",
    "    state = torch.tensor(state, device=device).float().unsqueeze(0)\n",
    "    current_ep_reward += reward\n",
    "    \n",
    "    if done:\n",
    "        ep_count += 1\n",
    "        scores.append(current_ep_reward)\n",
    "        current_ep_reward = 0\n",
    "        state, done = env.reset(), False  # Reset for the next episode\n",
    "        state = torch.tensor(state, device=device).float().unsqueeze(0)\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
